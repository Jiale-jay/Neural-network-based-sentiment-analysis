{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05fdc69f-7ff3-44d1-b55e-fdbc6eb91049",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataset\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcsv\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'vocab'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import dataset\n",
    "from vocab import Vocab\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7ac6f82-33d5-463e-9f78-a7853267530a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataset_train\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset_train'"
     ]
    }
   ],
   "source": [
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, train=True, sequence_max_len=100):\n",
    "        self.sequence_max_len = sequence_max_len\n",
    "        self.data_path = r\"./stsb-en-train.csv\" if train else r\"./stsb-en-test.csv\"\n",
    "        self.data_set = []\n",
    "        # 逐行读取csv文件\n",
    "        with open(self.data_path, 'r', encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for header_row in reader:\n",
    "                self.data_set.append(header_row)\n",
    "        self.threshold = 3.0\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # 从self.data_set获取评论并分词\n",
    "        sentences = self.data_set[idx]\n",
    "        review1 = dataset_train.tokenlize(sentences[0])\n",
    "        review2 = dataset_train.tokenlize(sentences[1])\n",
    "        return review1, review2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行处理\n",
    "    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]\n",
    "    :return: 元组\n",
    "    \"\"\"\n",
    "    # reviews, labels = zip(*batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_dataloader(train=True):\n",
    "    imdb_dataset = ImdbDataset(train, sequence_max_len=100)\n",
    "    my_dataloader = DataLoader(imdb_dataset, batch_size=200, shuffle=True, collate_fn=collate_fn)\n",
    "    return my_dataloader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ws = Vocab()\n",
    "    dl_train = get_dataloader(True)\n",
    "    dl_test = get_dataloader(False)\n",
    "    cnt = 1\n",
    "    for reviews in tqdm(dl_train, total=len(dl_train)):\n",
    "        # 一个sentence是一对具有一定相似度的语句\n",
    "        for sentence1, sentence2 in reviews:\n",
    "            ws.fit(sentence1)\n",
    "            ws.fit(sentence2)\n",
    "    \n",
    "\n",
    "    for reviews in tqdm(dl_test, total=len(dl_test)):\n",
    "        for sentence1, sentence2 in reviews:\n",
    "            ws.fit(sentence1)\n",
    "            ws.fit(sentence2)\n",
    "\n",
    "    ws.build_vocab()\n",
    "    print(len(ws))\n",
    "    if not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "    pickle.dump(ws, open(\"./models/vocab.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96976b-578e-4c30-9180-3978e4375661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import dataset_train\n",
    "from vocab import Vocab\n",
    "import csv\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, train=True, sequence_max_len=100):\n",
    "        self.sequence_max_len = sequence_max_len\n",
    "        self.data_path = r\"./stsb-en-train.csv\" if train else r\"./stsb-en-test.csv\"\n",
    "        self.data_set = []\n",
    "        # 逐行读取csv文件\n",
    "        with open(self.data_path, 'r', encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for header_row in reader:\n",
    "                self.data_set.append(header_row)\n",
    "        self.threshold = 3.0\n",
    "            \n",
    "    def __getitem__(self, idx):\n",
    "        # 从self.data_set获取评论并分词\n",
    "        sentences = self.data_set[idx]\n",
    "        review1 = dataset_train.tokenlize(sentences[0])\n",
    "        review2 = dataset_train.tokenlize(sentences[1])\n",
    "        return review1, review2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行处理\n",
    "    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]\n",
    "    :return: 元组\n",
    "    \"\"\"\n",
    "    # reviews, labels = zip(*batch)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def get_dataloader(train=True):\n",
    "    imdb_dataset = ImdbDataset(train, sequence_max_len=100)\n",
    "    my_dataloader = DataLoader(imdb_dataset, batch_size=200, shuffle=True, collate_fn=collate_fn)\n",
    "    return my_dataloader\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    ws = Vocab()\n",
    "    dl_train = get_dataloader(True)\n",
    "    dl_test = get_dataloader(False)\n",
    "    cnt = 1\n",
    "    for reviews in tqdm(dl_train, total=len(dl_train)):\n",
    "        # 一个sentence是一对具有一定相似度的语句\n",
    "        for sentence1, sentence2 in reviews:\n",
    "            ws.fit(sentence1)\n",
    "            ws.fit(sentence2)\n",
    "    \n",
    "\n",
    "    for reviews in tqdm(dl_test, total=len(dl_test)):\n",
    "        for sentence1, sentence2 in reviews:\n",
    "            ws.fit(sentence1)\n",
    "            ws.fit(sentence2)\n",
    "\n",
    "    ws.build_vocab()\n",
    "    print(len(ws))\n",
    "    if not os.path.exists(\"./models\"):\n",
    "        os.makedirs(\"./models\")\n",
    "    pickle.dump(ws, open(\"./models/vocab.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64aee4-b993-43e8-8e77-9959d9db1596",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from vocab import Vocab\n",
    "import csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eed953-f67c-4edc-bf5a-4c6add4f94da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Vocab()\n",
    "\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, train=True, sequence_max_len=100):\n",
    "        self.sequence_max_len = sequence_max_len\n",
    "        self.data_path = r\"./stsb-en-train.csv\" if train else r\"./stsb-en-test.csv\"\n",
    "        self.data_set = []\n",
    "        # 逐行读取csv文件\n",
    "        with open(self.data_path, 'r', encoding=\"utf-8\") as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for header_row in reader:\n",
    "                self.data_set.append(header_row)\n",
    "        self.threshold = 3.0\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 从self.data_set获取评论并分词\n",
    "        sentences = self.data_set[idx]\n",
    "        review1 = tokenlize(sentences[0])\n",
    "        review2 = tokenlize(sentences[1])\n",
    "        # 获取句子的相似度score\n",
    "        score = float(sentences[2])\n",
    "        label = 1 if score >= self.threshold else 0\n",
    "        return review1, review2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_set)\n",
    "\n",
    "\n",
    "def tokenlize(sentence):\n",
    "    \"\"\"\n",
    "    进行文本分词\n",
    "    :param sentence: str\n",
    "    :return: [str,str,str]\n",
    "    \"\"\"\n",
    "    fileters = ['!', '\"', '#', '$', '%', '&', '\\(', '\\)', '\\*', '\\+', ',', '-', '\\.', '/', ':', ';', '<', '=', '>',\n",
    "                '\\?', '@', '\\[', '\\\\', '\\]', '^', '_', '`', '\\{', '\\|', '\\}', '~', '\\t', '\\n', '\\x97', '\\x96', '”',\n",
    "                '“', ]\n",
    "    sentence = sentence.lower()  # 把大写转化为小写\n",
    "    sentence = re.sub(\"<br />\", \" \", sentence)\n",
    "    sentence = re.sub(\"|\".join(fileters), \" \", sentence)\n",
    "    result = [i for i in sentence.split(\" \") if len(i) > 0]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "# 以下为调试代码\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行处理\n",
    "    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]\n",
    "    :return: 元组\n",
    "    \"\"\"\n",
    "    reviews1, reviews2, labels = zip(*batch)\n",
    "\n",
    "    return reviews1, reviews2, labels\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    imdb_dataset = ImdbDataset(True)\n",
    "    my_dataloader = DataLoader(imdb_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "    for data in my_dataloader:\n",
    "        print(data)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "baccc7b4-34db-413d-bdf6-ecb808f96bef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset_train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdataset_train\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mvocab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Vocab\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dataset_train'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import dataset_train\n",
    "from vocab import Vocab\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddcab2-a885-4efe-ac8c-5f42e3718a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Vocab()\n",
    "\n",
    "train_batch_size = 512\n",
    "test_batch_size = 128\n",
    "sequence_max_len = 100\n",
    "embedding_dim = 200\n",
    "vocab = pickle.load(open(\"./models/vocab.pkl\", \"rb\"))\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    对batch数据进行处理\n",
    "    :param batch: [一个getitem的结果,getitem的结果,getitem的结果]\n",
    "    :return: 元组\n",
    "    \"\"\"\n",
    "    reviews1, reviews2, labels = zip(*batch)\n",
    "    reviews1 = torch.LongTensor([vocab.transform(i, max_len=sequence_max_len) for i in reviews1])\n",
    "    reviews2 = torch.LongTensor([vocab.transform(i, max_len=sequence_max_len) for i in reviews2])\n",
    "    labels = torch.LongTensor(labels)\n",
    "    return reviews1, reviews2, labels\n",
    "\n",
    "# def get_dataset():\n",
    "#     return dataset_train.ImdbDataset(train)\n",
    "\n",
    "def get_dataloader(train=True):\n",
    "    imdb_dataset = dataset_train.ImdbDataset(train)\n",
    "    batch_size = train_batch_size if train else test_batch_size\n",
    "    return DataLoader(imdb_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "class ImdbModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImdbModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_embeddings=len(vocab), embedding_dim=embedding_dim, padding_idx=vocab.PAD).to()\n",
    "        self.lstm = nn.LSTM(input_size=200, hidden_size=64, num_layers=2, batch_first=True, bidirectional=True,\n",
    "                            dropout=0.5)\n",
    "        self.fc1 = nn.Linear(64 * 2, 64)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        \"\"\"\n",
    "        :param input:[batch_size,max_len]\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input_embeded1 = self.embedding(input1)  # input embeded :[batch_size,max_len,200]\n",
    "        input_embeded2 = self.embedding(input2)\n",
    "\n",
    "        output, (h_n1, c_n) = self.lstm(input_embeded1)  # h_n :[4,batch_size,hidden_size]\n",
    "        output, (h_n2, c_n) = self.lstm(input_embeded2)\n",
    "        # out :[batch_size,hidden_size*2]\n",
    "        out1 = torch.cat([h_n1[-1, :, :], h_n1[-2, :, :]], dim=-1)  # 拼接正向最后一个输出和反向最后一个输出\n",
    "        out2 = torch.cat([h_n2[-1, :, :], h_n2[-2, :, :]], dim=-1)\n",
    "        # 进行全连接\n",
    "        out_fc1_1 = self.fc1(out1)\n",
    "        out_fc1_2 = self.fc1(out2)\n",
    "        # 进行relu\n",
    "        out_fc1_relu1 = F.relu(out_fc1_1)\n",
    "        out_fc1_relu2 = F.relu(out_fc1_2)\n",
    "        # 全连接\n",
    "        out_fc2_1 = self.fc2(out_fc1_relu1)  # out :[batch_size,2]\n",
    "        out_fc2_2 = self.fc2(out_fc1_relu2)\n",
    "        final_out = torch.cosine_similarity(out_fc2_1, out_fc2_2, dim=1)\n",
    "        return final_out\n",
    "\n",
    "\n",
    "def device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "\n",
    "def train(imdb_model, epoch):\n",
    "    \"\"\"\n",
    "    :param imdb_model:\n",
    "    :param epoch:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    train_dataloader = get_dataloader(train=True)\n",
    "    optimizer = Adam(imdb_model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "    for i in range(epoch):\n",
    "        bar = tqdm(train_dataloader, total=len(train_dataloader))\n",
    "        for idx, (data1, data2, target) in enumerate(bar):\n",
    "            optimizer.zero_grad()\n",
    "            data1 = data1.to(device())\n",
    "            data2 = data2.to(device())\n",
    "            target = target.float().to(device())\n",
    "            output = imdb_model(data1, data2)\n",
    "            loss = criterion(output, target)\n",
    "            # loss = F.mse_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            bar.set_description(\"epcoh:{}  idx:{}   loss:{:.6f}\".format(i, idx, loss.item()))\n",
    "    # 保存模型\n",
    "    path_model = \"./models/lstm_model.pkl\"\n",
    "    torch.save(imdb_model, path_model)\n",
    "    # 保存模型参数\n",
    "    path_state_dict = \"./models/lstm_model_state_dict.pkl\"\n",
    "    net_state_dict = imdb_model.state_dict()\n",
    "    torch.save(net_state_dict, path_state_dict)\n",
    "\n",
    "\n",
    "def calculate_accuracy(predictions, targets, threshold):\n",
    "    predicted_labels = (predictions > threshold).astype(int)\n",
    "    true_labels = (targets > threshold).astype(int)\n",
    "    accuracy = np.mean(predicted_labels == true_labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def test(imdb_model):\n",
    "    threshold = 0.2\n",
    "    \"\"\"\n",
    "    验证模型\n",
    "    :param imdb_model:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    test_loss = 0\n",
    "    imdb_model.eval()\n",
    "    test_dataloader = get_dataloader(train=False)\n",
    "    predictions = []\n",
    "    targets = []\n",
    "\n",
    "    i = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data1, data2, target in tqdm(test_dataloader):\n",
    "            data1 = data1.to(device())\n",
    "            data2 = data2.to(device())\n",
    "            target = target.float().to(device())\n",
    "            output = imdb_model(data1, data2)\n",
    "            \n",
    "            loss = F.mse_loss(output, target)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # 将 predictions 和 targets 转换成 numpy\n",
    "            predicted_scores = output.cpu().numpy()\n",
    "            \n",
    "            true_scores = target.cpu().numpy()\n",
    "\n",
    "            predictions.extend(predicted_scores)\n",
    "            targets.extend(true_scores)\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    accuracy = calculate_accuracy(np.array(predictions), np.array(targets), threshold)\n",
    "\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} (Total num: {len(test_dataloader.dataset)} Threshold: {threshold})\")\n",
    "\n",
    "\n",
    "def cal_score(imdb_model):\n",
    "    predictions = []\n",
    "    targets= []\n",
    "    test_dataloader = get_dataloader(train=False)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data1, data2, target in tqdm(test_dataloader):\n",
    "            data1 = data1.to(device())\n",
    "            data2 = data2.to(device())\n",
    "            target = target.float().to(device())\n",
    "            output = imdb_model(data1, data2)\n",
    "\n",
    "            # 将 predictions 和 targets 转换为 numpy\n",
    "            predicted_scores = output.cpu().numpy()\n",
    "            true_score = target.cpu().numpy()\n",
    "\n",
    "            predictions.extend(predicted_scores)\n",
    "            targets.extend(true_score)\n",
    "    \n",
    "    # 计算基准分数(Mean)\n",
    "    predictions = np.array(predictions)\n",
    "    targets = np.array(targets)\n",
    "    target_score = np.mean(targets)\n",
    "    prediction_score = np.mean(predictions)\n",
    "\n",
    "    print(f\"Mean Score (Baseline): {target_score:.4f}\")\n",
    "    print(f\"Mean Score (prediction): {prediction_score:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # 训练\n",
    "    imdb_model = ImdbModel().to(device())\n",
    "    train(imdb_model, 10)\n",
    "    # 测试\n",
    "    # imdb_dataset = get_dataset()\n",
    "    path_lstm_model = \"./models/lstm_model.pkl\"\n",
    "    lstm_model = torch.load(path_lstm_model)\n",
    "    test(lstm_model)\n",
    "    cal_score(lstm_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0c53b8-c766-4ab7-8b82-602e8fa68123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb7ae69-c886-4d1a-a204-5ca071c8118d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
