{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd149934-cf8e-4720-8666-7c935e27586b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import tarfile\n",
    "import requests\n",
    "import numpy as np\n",
    "import paddle\n",
    "from paddle.nn import Embedding\n",
    "import paddle.nn.functional as F\n",
    "from paddle.nn import LSTM, Embedding, Dropout, Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9269049e-4990-4393-8419-6f4f6ffc18e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download():\n",
    "    # 通过python的requests类，下载存储在\n",
    "    # https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz的文件\n",
    "    corpus_url = \"https://dataset.bj.bcebos.com/imdb%2FaclImdb_v1.tar.gz\"\n",
    "    web_request = requests.get(corpus_url)\n",
    "    corpus = web_request.content\n",
    "\n",
    "    # 将下载的文件写在当前目录的aclImdb_v1.tar.gz文件内\n",
    "    with open(\"./aclImdb_v1.tar.gz\", \"wb\") as f:\n",
    "        f.write(corpus)\n",
    "    f.close()\n",
    "\n",
    "download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e90f2086-f50c-4cb9-adc4-e8f47a5096b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence 0, Zentropa has much in common with The Third Man, another noir-like film set among the rubble of postwar Europe. Like TTM, there is much inventive camera work. There is an innocent American who gets emotionally involved with a woman he doesn't really understand, and whose naivety is all the more striking in contrast with the natives.<br /><br />But I'd have to say that The Third Man has a more well-crafted storyline. Zentropa is a bit disjointed in this respect. Perhaps this is intentional: it is presented as a dream/nightmare, and making it too coherent would spoil the effect. <br /><br />This movie is unrelentingly grim--\"noir\" in more than one sense; one never sees the sun shine. Grim, but intriguing, and frightening.\n",
      "sentence 0, label 1\n",
      "sentence 1, Zentropa is the most original movie I've seen in years. If you like unique thrillers that are influenced by film noir, then this is just the right cure for all of those Hollywood summer blockbusters clogging the theaters these days. Von Trier's follow-ups like Breaking the Waves have gotten more acclaim, but this is really his best work. It is flashy without being distracting and offers the perfect combination of suspense and dark humor. It's too bad he decided handheld cameras were the wave of the future. It's hard to say who talked him away from the style he exhibits here, but it's everyone's loss that he went into his heavily theoretical dogma direction instead.\n",
      "sentence 1, label 1\n",
      "sentence 2, Lars Von Trier is never backward in trying out new techniques. Some of them are very original while others are best forgotten.<br /><br />He depicts postwar Germany as a nightmarish train journey. With so many cities lying in ruins, Leo Kessler a young American of German descent feels obliged to help in their restoration. It is not a simple task as he quickly finds out.<br /><br />His uncle finds him a job as a night conductor on the Zentropa Railway Line. His job is to attend to the needs of the passengers. When the shoes are polished a chalk mark is made on the soles. A terrible argument ensues when a passenger's shoes are not chalked despite the fact they have been polished. There are many allusions to the German fanaticism of adherence to such stupid details.<br /><br />The railway journey is like an allegory representing man's procession through life with all its trials and tribulations. In one sequence Leo dashes through the back carriages to discover them filled with half-starved bodies appearing to have just escaped from Auschwitz . These images, horrible as they are, are fleeting as in a dream, each with its own terrible impact yet unconnected.<br /><br />At a station called Urmitz Leo jumps from the train with a parceled bomb. In view of many by-standers he connects the bomb to the underside of a carriage. He returns to his cabin and makes a connection to a time clock. Later he jumps from the train (at high speed) and lies in the cool grass on a river bank. Looking at the stars above he decides that his job is to build and not destroy. Subsequently as he sees the train approaching a giant bridge he runs at breakneck speed to board the train and stop the clock. If you care to analyse the situation it is a completely impossible task. Quite ridiculous in fact. It could only happen in a dream.<br /><br />It's strange how one remembers little details such as a row of cups hanging on hooks and rattling away with the swaying of the train.<br /><br />Despite the fact that this film is widely acclaimed, I prefer Lars Von Trier's later films (Breaking the Waves and The Idiots). The bomb scene described above really put me off. Perhaps I'm a realist.\n",
      "sentence 2, label 1\n",
      "sentence 3, *Contains spoilers due to me having to describe some film techniques, so read at your own risk!*<br /><br />I loved this film. The use of tinting in some of the scenes makes it seem like an old photograph come to life. I also enjoyed the projection of people on a back screen. For instance, in one scene, Leopold calls his wife and she is projected behind him rather than in a typical split screen. Her face is huge in the back and Leo's is in the foreground.<br /><br />One of the best uses of this is when the young boys kill the Ravensteins on the train, a scene shot in an almost political poster style, with facial close ups. It reminded me of Battleship Potemkin, that intense constant style coupled with the spray of red to convey tons of horror without much gore. Same with the scene when Katharina finds her father dead in the bathtub...you can only see the red water on the side. It is one of the things I love about Von Trier, his understatement of horror, which ends up making it all the more creepy.<br /><br />The use of text in the film was unique, like when Leo's character is pushed by the word, \"Werewolf.\" I have never seen anything like that in a film.<br /><br />The use of black comedy in this film was well done. Ernst-Hugo Järegård is great as Leo's uncle. It brings up the snickers I got from his role in the Kingdom (Riget.) This humor makes the plotline of absurd anal retentiveness of train conductors against the terrible backdrop of WW2 and all the chaos, easier to take. It reminds me of Riget in the way the hospital administrator is trying to maintain a normalcy at the end of part one when everything is going crazy. It shows that some people are truly oblivious to the awful things happening around them. Yet some people, like Leo, are tuned in, but do nothing positive about it.<br /><br />The voice over, done expertly well by Max von Sydow, is amusing too. It draws you into the story and makes you jump into Leo's head, which at times is a scary place to be.<br /><br />The movie brings up the point that one is a coward if they don't choose a side. I see the same idea used in Dancer in the Dark, where Bjork's character doesn't speak up for herself and ends up being her own destruction. Actually, at one time, Von Trier seemed anti-woman to me, by making Breaking the Waves and Dancer, but now I know his male characters don't fare well either! I found myself at the same place during the end of Dancer, when you seriously want the main character to rethink their actions, but of course, they never do!\n",
      "sentence 3, label 1\n",
      "sentence 4, That was the first thing that sprang to mind as I watched the closing credits to Europa make there was across the screen, never in my entire life have I seen a film of such technical genius, the visuals of Europa are so impressive that any film I watch in it's wake will only pale in comparison, forget your Michael Bay, Ridley Scott slick Hollywood cinematography, Europa has more ethereal beauty than anything those two could conjure up in a million years. Now I'd be the first to hail Lars von Trier a genius just off the back of his films Breaking the Waves and Dancer in the Dark, but this is stupid, the fact that Europa has gone un-noticed by film experts for so long is a crime against cinema, whilst overrated rubbish like Crouching Tiger, Hidden Dragon and Life is Beautiful clean up at the academy awards (but what do the know) Europa has been hidden away, absent form video stores and (until recently) any British TV channels. <br /><br />The visuals in Europa are not MTV gloss; it's not a case of style over substance, its more a case of substance dictating style. Much like his first film The Element of Crime, von Trier uses the perspective of the main character to draw us into his world, and much like Element, the film begins with the main character (or in the case of Europa, we the audience) being hypnotized. As we move down the tracks, the voice of the Narrator (Max von Sydow) counts us down into a deep sleep, until we awake in Europa. This allows von Trier and his three cinematographers to pay with the conventions of time and imagery, there are many scenes in Europa when a character in the background, who is in black and white, will interact with a person in the foreground who will be colour, von Trier is trying to show us how much precedence the coloured item or person has over the plot, for instance, it's no surprise that the first shot of Leopold Kessler (Jean-marc Barr) is in colour, since he is the only character who's actions have superiority over the film. <br /><br />The performances are good, they may not be on par with performances in later von Trier films, but that's just because the images are sometimes so distracting that you don't really pick up on them the first time round. But I would like to point out the fantastic performance of Jean-Marc Barr in the lead role, whose blind idealism is slowly warn down by the two opposing sides, until he erupts in the films final act. Again, muck like The Element of Crime, the film ends with our hero unable to wake up from his nightmare state, left in this terrible place, with only the continuing narration of von Sydow to seal his fate. Europa is a tremendous film, and I cant help thinking what a shame that von Trier has abandoned this way of filming, since he was clearly one of the most talented visual directors working at that time, Europa, much like the rest of his cinematic cannon is filled with a wealth of iconic scenes. His dedication to composition and mise-en-scene is unrivalled, not to mention his use of sound and production design. But since his no-frills melodramas turned out to be Breaking the Waves and Dancer in the Dark then who can argue, but it does seems like a waste of an imaginative talent. 10/10\n",
      "sentence 4, label 1\n"
     ]
    }
   ],
   "source": [
    "def load_imdb(is_training):\n",
    "    data_set = []\n",
    "\n",
    "\n",
    "    \n",
    "    for label in [\"pos\", \"neg\"]:\n",
    "        with tarfile.open(\"./aclImdb_v1.tar.gz\") as tarf:\n",
    "            path_pattern = \"aclImdb/train/\" + label + \"/.*\\.txt$\" if is_training \\\n",
    "                else \"aclImdb/test/\" + label + \"/.*\\.txt$\"\n",
    "            path_pattern = re.compile(path_pattern)\n",
    "            tf = tarf.next()\n",
    "            while tf != None:\n",
    "                if bool(path_pattern.match(tf.name)):\n",
    "                    sentence = tarf.extractfile(tf).read().decode()\n",
    "                    sentence_label = 0 if label == 'neg' else 1\n",
    "                    data_set.append((sentence, sentence_label)) \n",
    "                tf = tarf.next()\n",
    "\n",
    "    return data_set\n",
    "\n",
    "train_corpus = load_imdb(True)\n",
    "test_corpus = load_imdb(False)\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"sentence %d, %s\" % (i, train_corpus[i][0]))    \n",
    "    print(\"sentence %d, label %d\" % (i, train_corpus[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e61909-e777-482a-9dbb-2fb61401dbff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['zentropa', 'has', 'much', 'in', 'common', 'with', 'the', 'third', 'man,', 'another', 'noir-like', 'film', 'set', 'among', 'the', 'rubble', 'of', 'postwar', 'europe.', 'like', 'ttm,', 'there', 'is', 'much', 'inventive', 'camera', 'work.', 'there', 'is', 'an', 'innocent', 'american', 'who', 'gets', 'emotionally', 'involved', 'with', 'a', 'woman', 'he', \"doesn't\", 'really', 'understand,', 'and', 'whose', 'naivety', 'is', 'all', 'the', 'more', 'striking', 'in', 'contrast', 'with', 'the', 'natives.<br', '/><br', '/>but', \"i'd\", 'have', 'to', 'say', 'that', 'the', 'third', 'man', 'has', 'a', 'more', 'well-crafted', 'storyline.', 'zentropa', 'is', 'a', 'bit', 'disjointed', 'in', 'this', 'respect.', 'perhaps', 'this', 'is', 'intentional:', 'it', 'is', 'presented', 'as', 'a', 'dream/nightmare,', 'and', 'making', 'it', 'too', 'coherent', 'would', 'spoil', 'the', 'effect.', '<br', '/><br', '/>this', 'movie', 'is', 'unrelentingly', 'grim--\"noir\"', 'in', 'more', 'than', 'one', 'sense;', 'one', 'never', 'sees', 'the', 'sun', 'shine.', 'grim,', 'but', 'intriguing,', 'and', 'frightening.'], 1), (['zentropa', 'is', 'the', 'most', 'original', 'movie', \"i've\", 'seen', 'in', 'years.', 'if', 'you', 'like', 'unique', 'thrillers', 'that', 'are', 'influenced', 'by', 'film', 'noir,', 'then', 'this', 'is', 'just', 'the', 'right', 'cure', 'for', 'all', 'of', 'those', 'hollywood', 'summer', 'blockbusters', 'clogging', 'the', 'theaters', 'these', 'days.', 'von', \"trier's\", 'follow-ups', 'like', 'breaking', 'the', 'waves', 'have', 'gotten', 'more', 'acclaim,', 'but', 'this', 'is', 'really', 'his', 'best', 'work.', 'it', 'is', 'flashy', 'without', 'being', 'distracting', 'and', 'offers', 'the', 'perfect', 'combination', 'of', 'suspense', 'and', 'dark', 'humor.', \"it's\", 'too', 'bad', 'he', 'decided', 'handheld', 'cameras', 'were', 'the', 'wave', 'of', 'the', 'future.', \"it's\", 'hard', 'to', 'say', 'who', 'talked', 'him', 'away', 'from', 'the', 'style', 'he', 'exhibits', 'here,', 'but', \"it's\", \"everyone's\", 'loss', 'that', 'he', 'went', 'into', 'his', 'heavily', 'theoretical', 'dogma', 'direction', 'instead.'], 1), (['lars', 'von', 'trier', 'is', 'never', 'backward', 'in', 'trying', 'out', 'new', 'techniques.', 'some', 'of', 'them', 'are', 'very', 'original', 'while', 'others', 'are', 'best', 'forgotten.<br', '/><br', '/>he', 'depicts', 'postwar', 'germany', 'as', 'a', 'nightmarish', 'train', 'journey.', 'with', 'so', 'many', 'cities', 'lying', 'in', 'ruins,', 'leo', 'kessler', 'a', 'young', 'american', 'of', 'german', 'descent', 'feels', 'obliged', 'to', 'help', 'in', 'their', 'restoration.', 'it', 'is', 'not', 'a', 'simple', 'task', 'as', 'he', 'quickly', 'finds', 'out.<br', '/><br', '/>his', 'uncle', 'finds', 'him', 'a', 'job', 'as', 'a', 'night', 'conductor', 'on', 'the', 'zentropa', 'railway', 'line.', 'his', 'job', 'is', 'to', 'attend', 'to', 'the', 'needs', 'of', 'the', 'passengers.', 'when', 'the', 'shoes', 'are', 'polished', 'a', 'chalk', 'mark', 'is', 'made', 'on', 'the', 'soles.', 'a', 'terrible', 'argument', 'ensues', 'when', 'a', \"passenger's\", 'shoes', 'are', 'not', 'chalked', 'despite', 'the', 'fact', 'they', 'have', 'been', 'polished.', 'there', 'are', 'many', 'allusions', 'to', 'the', 'german', 'fanaticism', 'of', 'adherence', 'to', 'such', 'stupid', 'details.<br', '/><br', '/>the', 'railway', 'journey', 'is', 'like', 'an', 'allegory', 'representing', \"man's\", 'procession', 'through', 'life', 'with', 'all', 'its', 'trials', 'and', 'tribulations.', 'in', 'one', 'sequence', 'leo', 'dashes', 'through', 'the', 'back', 'carriages', 'to', 'discover', 'them', 'filled', 'with', 'half-starved', 'bodies', 'appearing', 'to', 'have', 'just', 'escaped', 'from', 'auschwitz', '.', 'these', 'images,', 'horrible', 'as', 'they', 'are,', 'are', 'fleeting', 'as', 'in', 'a', 'dream,', 'each', 'with', 'its', 'own', 'terrible', 'impact', 'yet', 'unconnected.<br', '/><br', '/>at', 'a', 'station', 'called', 'urmitz', 'leo', 'jumps', 'from', 'the', 'train', 'with', 'a', 'parceled', 'bomb.', 'in', 'view', 'of', 'many', 'by-standers', 'he', 'connects', 'the', 'bomb', 'to', 'the', 'underside', 'of', 'a', 'carriage.', 'he', 'returns', 'to', 'his', 'cabin', 'and', 'makes', 'a', 'connection', 'to', 'a', 'time', 'clock.', 'later', 'he', 'jumps', 'from', 'the', 'train', '(at', 'high', 'speed)', 'and', 'lies', 'in', 'the', 'cool', 'grass', 'on', 'a', 'river', 'bank.', 'looking', 'at', 'the', 'stars', 'above', 'he', 'decides', 'that', 'his', 'job', 'is', 'to', 'build', 'and', 'not', 'destroy.', 'subsequently', 'as', 'he', 'sees', 'the', 'train', 'approaching', 'a', 'giant', 'bridge', 'he', 'runs', 'at', 'breakneck', 'speed', 'to', 'board', 'the', 'train', 'and', 'stop', 'the', 'clock.', 'if', 'you', 'care', 'to', 'analyse', 'the', 'situation', 'it', 'is', 'a', 'completely', 'impossible', 'task.', 'quite', 'ridiculous', 'in', 'fact.', 'it', 'could', 'only', 'happen', 'in', 'a', 'dream.<br', '/><br', \"/>it's\", 'strange', 'how', 'one', 'remembers', 'little', 'details', 'such', 'as', 'a', 'row', 'of', 'cups', 'hanging', 'on', 'hooks', 'and', 'rattling', 'away', 'with', 'the', 'swaying', 'of', 'the', 'train.<br', '/><br', '/>despite', 'the', 'fact', 'that', 'this', 'film', 'is', 'widely', 'acclaimed,', 'i', 'prefer', 'lars', 'von', \"trier's\", 'later', 'films', '(breaking', 'the', 'waves', 'and', 'the', 'idiots).', 'the', 'bomb', 'scene', 'described', 'above', 'really', 'put', 'me', 'off.', 'perhaps', \"i'm\", 'a', 'realist.'], 1), (['*contains', 'spoilers', 'due', 'to', 'me', 'having', 'to', 'describe', 'some', 'film', 'techniques,', 'so', 'read', 'at', 'your', 'own', 'risk!*<br', '/><br', '/>i', 'loved', 'this', 'film.', 'the', 'use', 'of', 'tinting', 'in', 'some', 'of', 'the', 'scenes', 'makes', 'it', 'seem', 'like', 'an', 'old', 'photograph', 'come', 'to', 'life.', 'i', 'also', 'enjoyed', 'the', 'projection', 'of', 'people', 'on', 'a', 'back', 'screen.', 'for', 'instance,', 'in', 'one', 'scene,', 'leopold', 'calls', 'his', 'wife', 'and', 'she', 'is', 'projected', 'behind', 'him', 'rather', 'than', 'in', 'a', 'typical', 'split', 'screen.', 'her', 'face', 'is', 'huge', 'in', 'the', 'back', 'and', \"leo's\", 'is', 'in', 'the', 'foreground.<br', '/><br', '/>one', 'of', 'the', 'best', 'uses', 'of', 'this', 'is', 'when', 'the', 'young', 'boys', 'kill', 'the', 'ravensteins', 'on', 'the', 'train,', 'a', 'scene', 'shot', 'in', 'an', 'almost', 'political', 'poster', 'style,', 'with', 'facial', 'close', 'ups.', 'it', 'reminded', 'me', 'of', 'battleship', 'potemkin,', 'that', 'intense', 'constant', 'style', 'coupled', 'with', 'the', 'spray', 'of', 'red', 'to', 'convey', 'tons', 'of', 'horror', 'without', 'much', 'gore.', 'same', 'with', 'the', 'scene', 'when', 'katharina', 'finds', 'her', 'father', 'dead', 'in', 'the', 'bathtub...you', 'can', 'only', 'see', 'the', 'red', 'water', 'on', 'the', 'side.', 'it', 'is', 'one', 'of', 'the', 'things', 'i', 'love', 'about', 'von', 'trier,', 'his', 'understatement', 'of', 'horror,', 'which', 'ends', 'up', 'making', 'it', 'all', 'the', 'more', 'creepy.<br', '/><br', '/>the', 'use', 'of', 'text', 'in', 'the', 'film', 'was', 'unique,', 'like', 'when', \"leo's\", 'character', 'is', 'pushed', 'by', 'the', 'word,', '\"werewolf.\"', 'i', 'have', 'never', 'seen', 'anything', 'like', 'that', 'in', 'a', 'film.<br', '/><br', '/>the', 'use', 'of', 'black', 'comedy', 'in', 'this', 'film', 'was', 'well', 'done.', 'ernst-hugo', 'järegård', 'is', 'great', 'as', \"leo's\", 'uncle.', 'it', 'brings', 'up', 'the', 'snickers', 'i', 'got', 'from', 'his', 'role', 'in', 'the', 'kingdom', '(riget.)', 'this', 'humor', 'makes', 'the', 'plotline', 'of', 'absurd', 'anal', 'retentiveness', 'of', 'train', 'conductors', 'against', 'the', 'terrible', 'backdrop', 'of', 'ww2', 'and', 'all', 'the', 'chaos,', 'easier', 'to', 'take.', 'it', 'reminds', 'me', 'of', 'riget', 'in', 'the', 'way', 'the', 'hospital', 'administrator', 'is', 'trying', 'to', 'maintain', 'a', 'normalcy', 'at', 'the', 'end', 'of', 'part', 'one', 'when', 'everything', 'is', 'going', 'crazy.', 'it', 'shows', 'that', 'some', 'people', 'are', 'truly', 'oblivious', 'to', 'the', 'awful', 'things', 'happening', 'around', 'them.', 'yet', 'some', 'people,', 'like', 'leo,', 'are', 'tuned', 'in,', 'but', 'do', 'nothing', 'positive', 'about', 'it.<br', '/><br', '/>the', 'voice', 'over,', 'done', 'expertly', 'well', 'by', 'max', 'von', 'sydow,', 'is', 'amusing', 'too.', 'it', 'draws', 'you', 'into', 'the', 'story', 'and', 'makes', 'you', 'jump', 'into', \"leo's\", 'head,', 'which', 'at', 'times', 'is', 'a', 'scary', 'place', 'to', 'be.<br', '/><br', '/>the', 'movie', 'brings', 'up', 'the', 'point', 'that', 'one', 'is', 'a', 'coward', 'if', 'they', \"don't\", 'choose', 'a', 'side.', 'i', 'see', 'the', 'same', 'idea', 'used', 'in', 'dancer', 'in', 'the', 'dark,', 'where', \"bjork's\", 'character', \"doesn't\", 'speak', 'up', 'for', 'herself', 'and', 'ends', 'up', 'being', 'her', 'own', 'destruction.', 'actually,', 'at', 'one', 'time,', 'von', 'trier', 'seemed', 'anti-woman', 'to', 'me,', 'by', 'making', 'breaking', 'the', 'waves', 'and', 'dancer,', 'but', 'now', 'i', 'know', 'his', 'male', 'characters', \"don't\", 'fare', 'well', 'either!', 'i', 'found', 'myself', 'at', 'the', 'same', 'place', 'during', 'the', 'end', 'of', 'dancer,', 'when', 'you', 'seriously', 'want', 'the', 'main', 'character', 'to', 'rethink', 'their', 'actions,', 'but', 'of', 'course,', 'they', 'never', 'do!'], 1), (['that', 'was', 'the', 'first', 'thing', 'that', 'sprang', 'to', 'mind', 'as', 'i', 'watched', 'the', 'closing', 'credits', 'to', 'europa', 'make', 'there', 'was', 'across', 'the', 'screen,', 'never', 'in', 'my', 'entire', 'life', 'have', 'i', 'seen', 'a', 'film', 'of', 'such', 'technical', 'genius,', 'the', 'visuals', 'of', 'europa', 'are', 'so', 'impressive', 'that', 'any', 'film', 'i', 'watch', 'in', \"it's\", 'wake', 'will', 'only', 'pale', 'in', 'comparison,', 'forget', 'your', 'michael', 'bay,', 'ridley', 'scott', 'slick', 'hollywood', 'cinematography,', 'europa', 'has', 'more', 'ethereal', 'beauty', 'than', 'anything', 'those', 'two', 'could', 'conjure', 'up', 'in', 'a', 'million', 'years.', 'now', \"i'd\", 'be', 'the', 'first', 'to', 'hail', 'lars', 'von', 'trier', 'a', 'genius', 'just', 'off', 'the', 'back', 'of', 'his', 'films', 'breaking', 'the', 'waves', 'and', 'dancer', 'in', 'the', 'dark,', 'but', 'this', 'is', 'stupid,', 'the', 'fact', 'that', 'europa', 'has', 'gone', 'un-noticed', 'by', 'film', 'experts', 'for', 'so', 'long', 'is', 'a', 'crime', 'against', 'cinema,', 'whilst', 'overrated', 'rubbish', 'like', 'crouching', 'tiger,', 'hidden', 'dragon', 'and', 'life', 'is', 'beautiful', 'clean', 'up', 'at', 'the', 'academy', 'awards', '(but', 'what', 'do', 'the', 'know)', 'europa', 'has', 'been', 'hidden', 'away,', 'absent', 'form', 'video', 'stores', 'and', '(until', 'recently)', 'any', 'british', 'tv', 'channels.', '<br', '/><br', '/>the', 'visuals', 'in', 'europa', 'are', 'not', 'mtv', 'gloss;', \"it's\", 'not', 'a', 'case', 'of', 'style', 'over', 'substance,', 'its', 'more', 'a', 'case', 'of', 'substance', 'dictating', 'style.', 'much', 'like', 'his', 'first', 'film', 'the', 'element', 'of', 'crime,', 'von', 'trier', 'uses', 'the', 'perspective', 'of', 'the', 'main', 'character', 'to', 'draw', 'us', 'into', 'his', 'world,', 'and', 'much', 'like', 'element,', 'the', 'film', 'begins', 'with', 'the', 'main', 'character', '(or', 'in', 'the', 'case', 'of', 'europa,', 'we', 'the', 'audience)', 'being', 'hypnotized.', 'as', 'we', 'move', 'down', 'the', 'tracks,', 'the', 'voice', 'of', 'the', 'narrator', '(max', 'von', 'sydow)', 'counts', 'us', 'down', 'into', 'a', 'deep', 'sleep,', 'until', 'we', 'awake', 'in', 'europa.', 'this', 'allows', 'von', 'trier', 'and', 'his', 'three', 'cinematographers', 'to', 'pay', 'with', 'the', 'conventions', 'of', 'time', 'and', 'imagery,', 'there', 'are', 'many', 'scenes', 'in', 'europa', 'when', 'a', 'character', 'in', 'the', 'background,', 'who', 'is', 'in', 'black', 'and', 'white,', 'will', 'interact', 'with', 'a', 'person', 'in', 'the', 'foreground', 'who', 'will', 'be', 'colour,', 'von', 'trier', 'is', 'trying', 'to', 'show', 'us', 'how', 'much', 'precedence', 'the', 'coloured', 'item', 'or', 'person', 'has', 'over', 'the', 'plot,', 'for', 'instance,', \"it's\", 'no', 'surprise', 'that', 'the', 'first', 'shot', 'of', 'leopold', 'kessler', '(jean-marc', 'barr)', 'is', 'in', 'colour,', 'since', 'he', 'is', 'the', 'only', 'character', \"who's\", 'actions', 'have', 'superiority', 'over', 'the', 'film.', '<br', '/><br', '/>the', 'performances', 'are', 'good,', 'they', 'may', 'not', 'be', 'on', 'par', 'with', 'performances', 'in', 'later', 'von', 'trier', 'films,', 'but', \"that's\", 'just', 'because', 'the', 'images', 'are', 'sometimes', 'so', 'distracting', 'that', 'you', \"don't\", 'really', 'pick', 'up', 'on', 'them', 'the', 'first', 'time', 'round.', 'but', 'i', 'would', 'like', 'to', 'point', 'out', 'the', 'fantastic', 'performance', 'of', 'jean-marc', 'barr', 'in', 'the', 'lead', 'role,', 'whose', 'blind', 'idealism', 'is', 'slowly', 'warn', 'down', 'by', 'the', 'two', 'opposing', 'sides,', 'until', 'he', 'erupts', 'in', 'the', 'films', 'final', 'act.', 'again,', 'muck', 'like', 'the', 'element', 'of', 'crime,', 'the', 'film', 'ends', 'with', 'our', 'hero', 'unable', 'to', 'wake', 'up', 'from', 'his', 'nightmare', 'state,', 'left', 'in', 'this', 'terrible', 'place,', 'with', 'only', 'the', 'continuing', 'narration', 'of', 'von', 'sydow', 'to', 'seal', 'his', 'fate.', 'europa', 'is', 'a', 'tremendous', 'film,', 'and', 'i', 'cant', 'help', 'thinking', 'what', 'a', 'shame', 'that', 'von', 'trier', 'has', 'abandoned', 'this', 'way', 'of', 'filming,', 'since', 'he', 'was', 'clearly', 'one', 'of', 'the', 'most', 'talented', 'visual', 'directors', 'working', 'at', 'that', 'time,', 'europa,', 'much', 'like', 'the', 'rest', 'of', 'his', 'cinematic', 'cannon', 'is', 'filled', 'with', 'a', 'wealth', 'of', 'iconic', 'scenes.', 'his', 'dedication', 'to', 'composition', 'and', 'mise-en-scene', 'is', 'unrivalled,', 'not', 'to', 'mention', 'his', 'use', 'of', 'sound', 'and', 'production', 'design.', 'but', 'since', 'his', 'no-frills', 'melodramas', 'turned', 'out', 'to', 'be', 'breaking', 'the', 'waves', 'and', 'dancer', 'in', 'the', 'dark', 'then', 'who', 'can', 'argue,', 'but', 'it', 'does', 'seems', 'like', 'a', 'waste', 'of', 'an', 'imaginative', 'talent.', '10/10'], 1)]\n",
      "[(['previous', 'reviewer', 'claudio', 'carvalho', 'gave', 'a', 'much', 'better', 'recap', 'of', 'the', \"film's\", 'plot', 'details', 'than', 'i', 'could.', 'what', 'i', 'recall', 'mostly', 'is', 'that', 'it', 'was', 'just', 'so', 'beautiful,', 'in', 'every', 'sense', '-', 'emotionally,', 'visually,', 'editorially', '-', 'just', 'gorgeous.<br', '/><br', '/>if', 'you', 'like', 'movies', 'that', 'are', 'wonderful', 'to', 'look', 'at,', 'and', 'also', 'have', 'emotional', 'content', 'to', 'which', 'that', 'beauty', 'is', 'relevant,', 'i', 'think', 'you', 'will', 'be', 'glad', 'to', 'have', 'seen', 'this', 'extraordinary', 'and', 'unusual', 'work', 'of', 'art.<br', '/><br', '/>on', 'a', 'scale', 'of', '1', 'to', '10,', \"i'd\", 'give', 'it', 'about', 'an', '8.75.', 'the', 'only', 'reason', 'i', 'shy', 'away', 'from', '9', 'is', 'that', 'it', 'is', 'a', 'mood', 'piece.', 'if', 'you', 'are', 'in', 'the', 'mood', 'for', 'a', 'really', 'artistic,', 'very', 'romantic', 'film,', 'then', \"it's\", 'a', '10.', 'i', 'definitely', 'think', \"it's\", 'a', 'must-see,', 'but', 'none', 'of', 'us', 'can', 'be', 'in', 'that', 'mood', 'all', 'the', 'time,', 'so,', 'overall,', '8.75.'], 1), (['contains', '\"spoiler\"', 'information.', 'watch', 'this', \"director's\", 'other', 'film,', '\"earth\",', 'at', 'some', 'point.', \"it's\", 'a', 'better', 'film,', 'but', 'this', 'one', \"isn't\", 'bad', 'just', 'different.<br', '/><br', '/>a', 'rare', 'feminist', 'point', 'of', 'view', 'from', 'an', 'indian', 'filmmaker.', 'tradition,', 'rituals,', 'duty,', 'secrets,', 'and', 'the', 'portrayal', 'of', 'strict', 'sex', 'roles', 'make', 'this', 'an', 'engaging', 'and', 'culturally', 'dynamic', 'film', 'viewing', 'experience.', 'all', 'of', 'the', 'married', 'characters', 'lack', 'the', '\"fire\"', 'of', 'the', 'marriage', 'bed', 'with', 'their', 'respective', 'spouses.', 'one', 'husband', 'is', 'celibate', 'and', 'commits', 'a', 'form', 'of', 'spiritual', '\"adultery\"', 'by', 'giving', 'all', 'of', 'his', 'love,', 'honor,', 'time', 'and', 'respect', 'to', 'his', 'religious', 'swami', '(guru).', 'his', 'wife', 'is', 'lonely', 'and', 'yearns', 'for', 'intimacy', 'and', 'tenderness', 'which', 'she', 'eventually', 'finds', 'with', 'her', 'closeted', 'lesbian', 'sister-in-law', 'who', 'comes', 'to', 'live', 'in', 'their', 'house', 'with', 'her', 'unfaithful', 'husband.', 'this', 'unfaithful', 'husband', 'is', 'openly', 'in', 'love', 'with', 'his', 'chinese', 'mistress', 'but', 'was', 'forced', 'into', 'marriage', 'with', 'a', '(unbeknownest', 'to', 'him)', 'lesbian.', 'they', 'only', 'have', 'sex', 'once', 'when', 'his', 'closet', 'lesbian', 'wife', 'loses', 'her', 'virginity.<br', '/><br', '/>a', 'servant', 'lives', 'in', 'the', 'house', 'and', 'he', 'eventually', 'reveals', 'the', 'secret', 'that', 'the', 'two', 'women', 'are', 'lovers.', 'another', 'significant', 'character', 'is', 'the', 'elderly', 'matriarch', 'who', 'is', 'unable', 'to', 'speak', 'or', 'care', 'for', 'herself', 'due', 'to', 'a', 'stroke.', 'however,', 'she', 'uses', 'a', 'ringing', 'bell', 'to', 'communicate', 'her', 'needs', 'as', 'well', 'as', 'her', 'displeasure', 'with', 'the', 'family', 'members.', 'she', 'lets', 'them', 'know', 'through', 'her', 'bell', 'or', 'by', 'pounding', 'her', 'fist', 'that', 'she', 'knows', 'exacly', \"what's\", 'going', 'on', 'in', 'the', 'house', 'and', 'how', 'much', 'she', 'disapproves.<br', '/><br', '/>in', 'the', 'end,', 'the', 'truth', 'about', 'everybody', 'comes', 'out', 'and', 'the', 'two', 'female', 'lovers', 'end', 'up', 'running', 'away', 'together.', 'but,', 'not', 'before', 'there', 'is', 'an', 'emotional', 'scene', 'between', 'the', 'swami-addicted', 'husband', 'and', 'his', 'formerly', 'straight', 'wife.', 'her', 'sari', 'catches', 'on', 'fire', 'and', 'at', 'first', 'we', 'think', 'she', 'is', 'going', 'to', 'die.', 'however,', 'we', 'see', 'the', 'two', 'women', 'united', 'in', 'the', 'very', 'last', 'scene', 'of', 'the', 'movie.<br', '/><br', '/>the', 'writer/director', 'of', 'this', 'film', 'challenges', 'her', \"culture's\", 'traditions,', 'but', 'she', 'shows', 'us', 'individual', 'human', 'beings', 'who', 'are', 'trapped', 'by', 'their', 'culture', 'and', 'gender.', 'we', 'come', 'to', 'really', 'care', 'about', 'the', 'characters', 'and', 'we', \"don't\", 'see', 'them', 'as', 'stereotypes.', 'each', 'on', 'surprises', 'us', 'with', 'their', 'humanity,', 'vulgarity,', 'tenderness,', 'anger,', 'and', 'spirit.'], 1), (['this', 'is', 'my', 'first', 'deepa', 'mehta', 'film.', 'i', 'saw', 'the', 'film', 'on', 'tv', 'in', 'its', 'hindi', 'version', 'with', 'its', '\"sita\"', 'character', 'presented', 'as', 'nita.', 'i', 'also', 'note', 'that', 'it', 'is', 'radha', 'who', 'underwent', 'the', 'allegorical', 'trial', 'by', 'fire', 'in', 'the', 'film', 'and', 'not', 'nita/sita.', 'yet', 'what', 'i', 'loved', 'about', 'the', 'film', 'was', 'its', 'screenplay', 'by', 'ms', 'mehta,', 'not', 'her', 'direction.', 'the', 'characters,', 'big', 'and', 'small,', 'were', 'well-developed', 'and', 'seemed', 'quixotic', 'towards', 'the', 'end--somewhat', 'like', 'the', 'end', 'of', \"mazursky's\", '\"an', 'unmarried', 'woman.\"', 'they', 'are', 'brave', 'women', 'surrounded', 'by', 'cardboard', 'men.', 'and', 'one', 'cardboard', 'man', '(ashok)', 'seems', 'to', 'come', 'alive', 'in', 'the', 'last', 'shot', 'we', 'see', 'of', 'him---carrying', 'his', 'invalid', 'mother', 'biji.', 'he', 'seems', 'to', 'finally', 'take', 'on', 'a', 'future', 'responsibility', 'beyond', 'celibacy', 'and', 'adherance', 'to', 'religion.', '<br', '/><br', '/>ms', 'mehta', 'seems', 'to', 'fumble', 'as', 'a', 'director', '(however,', 'compared', 'to', 'most', 'indian', 'mainstream', 'cinema', 'she', 'would', 'seem', 'to', 'be', 'brilliant)', 'as', 'she', 'cannot', 'use', 'her', 'script', 'to', 'go', 'beyond', 'the', 'microscopic', 'joint', 'family', 'she', 'is', 'presenting', 'except', 'presenting', 'a', 'glimpse', 'of', 'the', 'chinese', 'micro-minority', 'in', 'the', 'social', 'milieu', 'of', 'india.', 'she', 'even', 'dedicates', 'the', 'film', 'to', 'her', 'mother', 'and', 'daughter', '(not', 'her', 'father!)', 'yet', 'her', 'radha', 'reminesces', 'of', 'halcyon', 'days', 'with', 'both', 'her', 'parents', 'in', 'a', 'mustard', 'field.', 'compare', 'her', 'to', 'mrinal', 'sen,', 'adoor', 'gopalakrishnan,', 'muzaffar', 'ali', 'and', 'she', 'is', 'dwarfed', 'by', 'these', 'giants--given', 'her', 'competent', 'canadian', 'production', 'team', 'and', 'financial', 'resources!', \"mehta's\", 'film', 'of', 'two', 'bisexual', 'ladies', 'in', 'an', 'indian', 'middle-class', 'household', 'may', 'be', 'sacrilege', 'to', 'some,', 'but', 'merely', 'captures', 'the', 'atrophy', 'of', 'middle-class', 'homes', 'that', 'does', 'not', 'seem', 'to', 'aspire', 'for', 'something', 'better', 'than', 'its', 'immediate', 'survival', 'in', 'a', 'limited', 'social', 'space.', 'kannada,', 'malayalam,', 'and', 'bengali', 'films', 'have', 'touched', 'parallel', 'themes', 'in', 'india', 'but', 'did', 'not', 'have', 'the', 'publicity', 'that', 'surrounded', 'this', 'film', 'and', 'therefore', 'have', 'not', 'been', 'seen', 'by', 'a', 'wide', 'segment', 'of', 'knowledgeable', 'cinemagoers.<br', '/><br', '/>ms', 'das,', 'ms', 'azmi,', 'mr', 'jafri', 'and', 'mr', 'kharbanda', 'are', 'credible', 'but', 'not', 'outstanding.', 'ms', 'azmi', 'is', 'a', 'talented', 'actress', 'who', 'gave', 'superb', 'performances', 'under', 'good', 'directors', '(mrinal', \"sen's\", '\"khandar\",', 'gautam', \"ghose's\", '\"paar\",', \"benegal's\", '\"ankur\")', 'a', 'brilliance', 'notably', 'absent', 'in', 'this', 'film.', 'ms', 'das', 'sparkled', 'due', 'to', 'her', 'screen', 'presence', 'rather', 'than', 'her', 'acting', 'capability.', 'all', 'in', 'all,', 'the', \"film's\", 'strength', 'remains', 'in', 'the', 'structure', 'of', 'the', 'screenplay', 'which', 'is', 'above', 'average', 'in', 'terms', 'of', 'international', 'cinema.', 'i', 'am', 'sure', 'ms', 'mehta', 'can', 'hone', 'her', 'writing', 'talents', 'in', 'her', 'future', 'screenplays.'], 1), (['this', 'was', 'a', 'great', 'film', 'in', 'every', 'sense', 'of', 'the', 'word.', 'it', 'tackles', 'the', 'subject', 'of', 'tribadism', 'in', 'a', 'society', 'that', 'is', 'quite', 'intolerant', 'of', 'any', 'deviations', 'from', 'the', 'norm.', 'it', 'criticises', 'a', 'great', 'many', 'indian', 'customs', 'that', 'many', 'find', 'oppressive', '--', 'such', 'as', 'the', 'arranging', 'of', 'marriages', 'by', 'others,', 'the', 'importance', 'of', 'status', 'and', 'face,', 'religious', 'hypocrisy,', 'sexism,', 'the', 'valuation', 'of', 'women', 'in', 'terms', 'of', 'their', 'baby-making', 'capacity,', 'the', 'binding', 'concepts', 'of', 'duty', 'and', 'so', 'on.', 'at', 'the', 'heart', 'of', 'the', 'film', 'is', 'a', 'touching', 'love', 'story', 'that', 'goes', 'beyond', 'such', 'limitations', 'of', 'the', 'society', 'which', 'the', 'two', 'protagonists', 'find', 'themselves.', 'the', 'film', 'is', 'well-acted', 'and', 'genuine,', 'completely', 'believable', 'from', 'beginning', 'to', 'end,', 'unlike', 'most', 'bollywood', 'flicks.', 'the', 'main', 'faults', 'of', 'the', 'film', 'as', 'i', 'saw', 'it', 'was', 'first,', 'that', 'the', 'two', 'lovers', 'seem', 'drawn', 'to', 'one', 'another', 'not', 'necessarily', 'by', 'a', 'natural', 'affinity', 'for', 'each', 'other', 'as', 'much', 'as', 'the', 'fact', 'that', 'they', 'are', 'stuck', 'in', 'dead-end', 'marriages', 'with', 'no', 'passion', 'and', 'no', 'rewards.', 'this', 'may', 'play', 'a', 'part', 'in', 'the', 'sexual', 'awakening', 'of', 'the', 'characters,', 'but', 'most', 'people', 'stuck', 'in', 'the', 'same', 'situation', 'will', 'not', '\"turn', 'homosexual\".', 'it', 'seems', 'clear', 'from', 'the', 'beginning', 'of', 'the', 'film', 'that', 'the', 'two', 'characters', 'are', 'quite', 'heterosexual', '--', 'when', 'radha', 'does', 'her', 'scene', 'at', 'the', 'end', 'of', 'the', 'movie', 'with', 'aashok,', 'she', 'makes', 'it', 'quite', 'clear', 'that', '\"without', 'desire', 'she', 'was', 'dead\",', 'and', 'the', 'implication', 'was', 'that', 'if', 'he', 'had', 'desired', 'so,', 'he', 'could', 'have', 'fulfilled', 'her', 'quite', 'completely,', 'and', 'also', 'when', 'sita', 'seemed', 'very', 'disappointed', 'when', 'her', 'husband', 'seemed', 'to', 'not', 'like', 'her.', 'such', 'situations', 'do', 'not', 'turn', 'people', 'into', 'homosexuals', '--', 'they', 'may', 'seek', 'comfort', 'in', 'others', 'in', 'the', 'same', 'position,', 'but', 'inthe', 'film', 'it', 'is', 'not', 'at', 'all', 'made', 'clear', 'that', 'they', 'are', 'lesbians', 'from', 'the', 'beginning', '--', 'quite', 'the', 'opposite.', 'some', 'people', 'are', 'bisexual,', 'it', 'is', 'true,', 'but', 'most', 'tend', 'to', 'be', 'either', 'hetero-', 'or', 'homosexual.', 'in', 'the', 'case', 'of', 'the', 'ladies', 'in', 'the', 'film,', 'both', 'had', 'insensitive', 'jerks', 'for', 'husbands', '.', '.', '.', 'if', 'this', 'had', 'not', 'been', 'the', 'case,', 'would', 'they', 'have', 'naturally', 'found', 'the', 'need', 'to', 'express', 'their', 'desire', 'in', 'a', 'relationship', 'that', 'they', 'may', 'have', 'otherwise', 'not', 'have', 'considered?', 'the', 'film', 'ignores', 'this.', 'the', 'other', 'fault', 'is', 'the', 'naming', 'of', 'the', 'characters', '.', '.', '.', 'the', 'names', 'sita', 'and', 'radha', 'seem', 'contrived', 'deliberately', 'to', 'shock', 'and', 'outrage', '(imagine', 'a', 'film', 'in', 'america', 'depicting', 'a', 'gay', 'relationship', 'between', 'a', 'man', 'named', '\"jesus\"', 'and', 'another', 'named', '\"paul\"!)', 'by', 'using', 'names', 'associated', 'with', 'various', 'hindoo', 'scriptures.', 'the', 'film', 'is', 'strong', 'enough', 'to', 'stand', 'on', 'its', 'own', 'and', 'needs', 'no', 'such', 'devices', 'in', 'my', 'opinion.', 'at', 'any', 'rate,', 'the', 'faults', 'do', 'not', 'take', 'much', 'away', 'from', 'the', 'power', 'of', 'the', 'movie.', 'it', 'is', 'indeed', 'a', 'very', 'touching', 'and', 'powerful', 'story', '--', 'the', 'images', 'and', 'characters', 'will', 'stay', 'with', 'you', 'a', 'long', 'time', 'after', 'you', 'leave', 'the', 'theatre.'], 1), (['a', 'stunningly', 'well-made', 'film,', 'with', 'exceptional', 'acting,', 'directing,', 'writing,', 'and', 'photography.<br', '/><br', '/>a', 'newlywed', 'finds', 'married', 'life', 'not', 'what', 'she', 'expected,', 'and', 'starts', 'to', 'question', 'her', 'duty', 'to', 'herself', 'versus', 'her', 'duty', 'to', 'society.', 'together', 'with', 'her', 'sister', '-in-law,', 'she', 'makes', 'some', 'radical', 'departures', 'from', 'conventional', 'roles', 'and', 'mores.'], 1)]\n"
     ]
    }
   ],
   "source": [
    "def data_preprocess(corpus):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "\n",
    "        sentence = sentence.strip().lower()\n",
    "        sentence = sentence.split(\" \")\n",
    "        \n",
    "        data_set.append((sentence, sentence_label))\n",
    "\n",
    "    return data_set\n",
    "\n",
    "train_corpus = data_preprocess(train_corpus)\n",
    "test_corpus = data_preprocess(test_corpus)\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed673bda-abcd-46e9-9fd9-428de16ae8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are totoally 252173 different words in the corpus\n",
      "word [oov], its id 0, its word freq 10000000000\n",
      "word [pad], its id 1, its word freq 10000000000\n",
      "word the, its id 2, its word freq 322174\n",
      "word a, its id 3, its word freq 159949\n",
      "word and, its id 4, its word freq 158556\n",
      "word of, its id 5, its word freq 144459\n",
      "word to, its id 6, its word freq 133965\n",
      "word is, its id 7, its word freq 104170\n",
      "word in, its id 8, its word freq 90521\n",
      "word i, its id 9, its word freq 70477\n"
     ]
    }
   ],
   "source": [
    "# 构造词典，统计每个词的频率，并根据频率将每个词转换为一个整数id\n",
    "def build_dict(corpus):\n",
    "    word_freq_dict = dict()\n",
    "    for sentence, _ in corpus:\n",
    "        for word in sentence:\n",
    "            if word not in word_freq_dict:\n",
    "                word_freq_dict[word] = 0\n",
    "            word_freq_dict[word] += 1\n",
    "\n",
    "    word_freq_dict = sorted(word_freq_dict.items(), key = lambda x:x[1], reverse = True)\n",
    "    \n",
    "    word2id_dict = dict()\n",
    "    word2id_freq = dict()\n",
    "\n",
    "    \n",
    "    word2id_dict['[oov]'] = 0\n",
    "    word2id_freq[0] = 1e10\n",
    "\n",
    "    word2id_dict['[pad]'] = 1\n",
    "    word2id_freq[1] = 1e10\n",
    "\n",
    "    for word, freq in word_freq_dict:\n",
    "        word2id_dict[word] = len(word2id_dict)\n",
    "        word2id_freq[word2id_dict[word]] = freq\n",
    "\n",
    "    return word2id_freq, word2id_dict\n",
    "\n",
    "word2id_freq, word2id_dict = build_dict(train_corpus)\n",
    "vocab_size = len(word2id_freq)\n",
    "print(\"there are totoally %d different words in the corpus\" % vocab_size)\n",
    "for _, (word, word_id) in zip(range(10), word2id_dict.items()):\n",
    "    print(\"word %s, its id %d, its word freq %d\" % (word, word_id, word2id_freq[word_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3186d4f-b0e0-45a4-93f9-bdb081ceec84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 tokens in the corpus\n",
      "[([22216, 41, 76, 8, 1136, 17, 2, 874, 979, 167, 69425, 24, 283, 707, 2, 19881, 5, 16628, 11952, 37, 100421, 52, 7, 76, 5733, 415, 912, 52, 7, 32, 1426, 299, 36, 195, 2299, 644, 17, 3, 282, 27, 141, 61, 7447, 4, 555, 25364, 7, 35, 2, 51, 3590, 8, 2691, 17, 2, 69426, 13, 688, 428, 26, 6, 142, 11, 2, 874, 160, 41, 3, 51, 14841, 4458, 22216, 7, 3, 218, 6262, 8, 10, 6919, 382, 10, 7, 100422, 12, 7, 1394, 15, 3, 100423, 4, 242, 12, 104, 5041, 54, 2368, 2, 4828, 109, 13, 255, 20, 7, 32280, 100424, 8, 51, 68, 30, 29571, 30, 102, 1010, 2, 4142, 18952, 11069, 18, 11636, 4, 12644], 1), ([22216, 7, 2, 78, 225, 20, 190, 119, 8, 1043, 46, 25, 37, 1008, 4578, 11, 22, 4379, 31, 24, 9244, 96, 10, 7, 39, 2, 246, 5601, 16, 35, 5, 136, 385, 1901, 11953, 69427, 2, 3689, 124, 2351, 2666, 17339, 100425, 37, 2732, 2, 6821, 26, 1702, 51, 35630, 18, 10, 7, 61, 21, 116, 912, 12, 7, 7006, 191, 99, 6263, 4, 1485, 2, 439, 2239, 5, 1221, 4, 513, 2598, 44, 104, 97, 27, 761, 32281, 5417, 66, 2, 3893, 5, 2, 3263, 44, 261, 6, 142, 36, 3690, 101, 285, 34, 2, 507, 27, 13410, 611, 18, 44, 4535, 2398, 11, 27, 376, 72, 21, 2692, 25365, 15379, 583, 2649], 1), ([9839, 2666, 8704, 7, 102, 16629, 8, 243, 47, 154, 22217, 45, 5, 115, 22, 49, 225, 133, 498, 22, 116, 35631, 13, 3779, 4730, 16628, 3630, 15, 3, 9428, 1621, 10296, 17, 38, 100, 7214, 3515, 8, 27295, 6442, 20987, 3, 185, 299, 5, 1155, 7448, 673, 18114, 6, 323, 8, 59, 54956, 12, 7, 23, 3, 693, 3660, 15, 27, 1048, 554, 2877, 13, 7328, 1882, 554, 101, 3, 331, 15, 3, 394, 10539, 19, 2, 22216, 14842, 3720, 21, 331, 7, 6, 5370, 6, 2, 676, 5, 2, 46152, 50, 2, 5663, 22, 6264, 3, 13411, 1132, 7, 95, 19, 2, 100426, 3, 495, 4829, 10798, 50, 3, 54957, 5663, 22, 23, 100427, 446, 2, 231, 33, 26, 71, 40128, 52, 22, 100, 13016, 6, 2, 1155, 35632, 5, 32282, 6, 130, 467, 29572, 13, 93, 14842, 1537, 7, 37, 32, 13809, 9626, 1535, 46153, 140, 161, 17, 35, 83, 7108, 4, 46154, 8, 30, 809, 6442, 27296, 140, 2, 149, 69428, 6, 1874, 115, 1079, 17, 100428, 3122, 3608, 6, 26, 39, 4179, 34, 54958, 421, 124, 8126, 668, 15, 33, 1927, 22, 10799, 15, 8, 3, 9044, 233, 17, 83, 209, 495, 1769, 266, 100429, 13, 2352, 3, 2255, 437, 100430, 6442, 2994, 34, 2, 1621, 17, 3, 100431, 11351, 8, 797, 5, 100, 54959, 27, 11954, 2, 3161, 6, 2, 40129, 5, 3, 69429, 27, 1883, 6, 21, 3609, 4, 146, 3, 2123, 6, 3, 84, 35633, 370, 27, 2994, 34, 2, 1621, 2599, 317, 69430, 4, 1917, 8, 2, 845, 9429, 19, 3, 2618, 16630, 275, 29, 2, 482, 801, 27, 973, 11, 21, 331, 7, 6, 1712, 4, 23, 69431, 8871, 15, 27, 1010, 2, 1621, 7329, 3, 1468, 4056, 27, 1108, 29, 40130, 2798, 6, 2667, 2, 1621, 4, 595, 2, 35633, 46, 25, 456, 6, 27297, 2, 1173, 12, 7, 3, 302, 1240, 13017, 168, 895, 8, 6083, 12, 90, 58, 783, 8, 3, 20988, 13, 792, 694, 79, 30, 7449, 105, 1724, 130, 15, 3, 5371, 5, 27298, 2378, 19, 12645, 4, 27299, 285, 17, 2, 23700, 5, 2, 32283, 13, 3837, 2, 231, 11, 10, 24, 7, 4980, 46155, 9, 2845, 9839, 2666, 17339, 370, 129, 100432, 2, 6821, 4, 2, 69432, 2, 3161, 150, 2152, 801, 61, 253, 87, 1308, 382, 145, 3, 69433], 1), ([69434, 2399, 610, 6, 87, 240, 6, 1613, 45, 24, 17340, 38, 308, 29, 111, 209, 100433, 13, 165, 403, 10, 139, 2, 316, 5, 46156, 8, 45, 5, 2, 159, 146, 12, 268, 37, 32, 170, 10297, 208, 6, 506, 9, 81, 447, 2, 15380, 5, 89, 19, 3, 149, 1175, 16, 2890, 8, 30, 971, 11955, 2030, 21, 378, 4, 53, 7, 9627, 470, 101, 228, 68, 8, 3, 716, 3569, 1175, 42, 468, 7, 588, 8, 2, 149, 4, 23701, 7, 8, 2, 54960, 13, 1274, 5, 2, 116, 988, 5, 10, 7, 50, 2, 185, 1197, 494, 2, 100434, 19, 2, 10800, 3, 150, 339, 8, 32, 202, 954, 4459, 2329, 17, 2678, 547, 27300, 12, 1457, 87, 5, 35634, 54961, 11, 1760, 1750, 507, 6177, 17, 2, 14843, 5, 825, 6, 2909, 3414, 5, 213, 191, 76, 5093, 163, 17, 2, 150, 50, 40131, 554, 42, 410, 460, 8, 2, 100435, 64, 58, 67, 2, 825, 1522, 19, 2, 4880, 12, 7, 30, 5, 2, 194, 9, 120, 43, 2666, 69435, 21, 23702, 5, 2846, 63, 654, 65, 242, 12, 35, 2, 51, 32284, 13, 93, 316, 5, 3805, 8, 2, 24, 14, 7705, 37, 50, 23701, 122, 7, 3691, 31, 2, 4830, 100436, 9, 26, 102, 119, 237, 37, 11, 8, 3, 748, 13, 93, 316, 5, 328, 289, 8, 10, 24, 14, 107, 1682, 100437, 69436, 7, 88, 15, 23701, 18115, 12, 857, 65, 2, 69437, 9, 176, 34, 21, 263, 8, 2, 6716, 100438, 10, 641, 146, 2, 18116, 5, 2256, 15381, 100439, 5, 1621, 100440, 375, 2, 495, 5262, 5, 11070, 4, 35, 2, 23703, 3927, 6, 12301, 12, 1644, 87, 5, 25366, 8, 2, 108, 2, 2050, 54962, 7, 243, 6, 4611, 3, 20989, 29, 2, 172, 5, 182, 30, 50, 292, 7, 162, 7450, 12, 287, 11, 45, 89, 22, 319, 10050, 6, 2, 552, 194, 2022, 197, 429, 266, 45, 1051, 37, 19882, 22, 8705, 1441, 18, 85, 164, 1117, 43, 619, 13, 93, 642, 2780, 247, 6920, 107, 31, 2962, 2666, 46157, 7, 1383, 799, 12, 3751, 25, 72, 2, 75, 4, 146, 25, 2051, 72, 23701, 3692, 63, 29, 286, 7, 3, 861, 320, 6, 5418, 13, 93, 20, 857, 65, 2, 249, 11, 30, 7, 3, 11071, 46, 33, 86, 2210, 3, 4880, 9, 67, 2, 163, 321, 309, 8, 4928, 8, 2, 2650, 106, 54963, 122, 141, 1247, 65, 16, 907, 4, 654, 65, 99, 42, 209, 11637, 2781, 29, 30, 390, 2666, 8704, 401, 100441, 6, 493, 31, 242, 2732, 2, 6821, 4, 14298, 18, 203, 9, 118, 21, 950, 121, 86, 3516, 107, 16631, 9, 236, 608, 29, 2, 163, 320, 276, 2, 172, 5, 14298, 50, 25, 1106, 171, 2, 258, 122, 6, 19883, 59, 10540, 18, 5, 532, 33, 102, 14299], 1), ([11, 14, 2, 82, 174, 11, 46158, 6, 432, 15, 9, 273, 2, 2926, 1137, 6, 9245, 91, 52, 14, 563, 2, 2046, 102, 8, 57, 369, 161, 26, 9, 119, 3, 24, 5, 130, 1660, 7848, 2, 2978, 5, 9245, 22, 38, 1409, 11, 92, 24, 9, 110, 8, 44, 3752, 74, 58, 7573, 8, 15382, 885, 111, 481, 29573, 15978, 1223, 6084, 385, 2494, 9245, 41, 51, 14844, 1140, 68, 237, 136, 103, 90, 16632, 65, 8, 3, 1530, 1043, 203, 428, 28, 2, 82, 6, 13810, 9839, 2666, 8704, 3, 1792, 39, 137, 2, 149, 5, 21, 129, 2732, 2, 6821, 4, 4928, 8, 2, 2650, 18, 10, 7, 2353, 2, 231, 11, 9245, 41, 908, 69438, 31, 24, 12302, 16, 38, 229, 7, 3, 997, 375, 2443, 1828, 5311, 3280, 37, 15979, 13811, 1572, 3957, 4, 161, 7, 315, 2891, 65, 29, 2, 1864, 2799, 2112, 48, 85, 2, 20990, 9245, 41, 71, 1572, 2197, 7109, 926, 433, 8412, 4, 20991, 69439, 92, 632, 280, 19884, 109, 13, 93, 2978, 8, 9245, 22, 23, 6532, 100442, 44, 23, 3, 529, 5, 507, 127, 10298, 83, 51, 3, 529, 5, 3631, 27301, 3304, 76, 37, 21, 82, 24, 2, 1498, 5, 6533, 2666, 8704, 988, 2, 2927, 5, 2, 258, 122, 6, 2529, 188, 72, 21, 1437, 4, 76, 37, 10801, 2, 24, 763, 17, 2, 258, 122, 875, 8, 2, 529, 5, 35635, 69, 2, 18953, 99, 46159, 15, 69, 858, 214, 2, 25367, 2, 642, 5, 2, 5792, 35636, 2666, 69440, 10051, 188, 214, 72, 3, 991, 11956, 313, 69, 6265, 8, 35637, 10, 1952, 2666, 8704, 4, 21, 284, 32285, 6, 966, 17, 2, 7574, 5, 84, 4, 10802, 52, 22, 100, 159, 8, 9245, 50, 3, 122, 8, 2, 7849, 36, 7, 8, 328, 4, 4057, 74, 7451, 17, 3, 450, 8, 2, 19885, 36, 74, 28, 18117, 2666, 8704, 7, 243, 6, 134, 188, 79, 76, 69441, 2, 32286, 8127, 40, 450, 41, 127, 2, 728, 16, 2890, 44, 60, 1200, 11, 2, 82, 339, 5, 11955, 20987, 54964, 46160, 7, 8, 18117, 227, 27, 7, 2, 58, 122, 808, 2184, 26, 19886, 127, 2, 139, 109, 13, 93, 423, 22, 523, 33, 186, 23, 28, 19, 3415, 17, 423, 8, 370, 2666, 8704, 580, 18, 189, 39, 77, 2, 1385, 22, 501, 38, 6263, 11, 25, 86, 61, 1281, 65, 19, 115, 2, 82, 84, 18118, 18, 9, 54, 37, 6, 249, 47, 2, 936, 269, 5, 22218, 19887, 8, 2, 475, 1469, 555, 2268, 18954, 7, 1512, 3006, 214, 31, 2, 103, 11957, 16633, 313, 27, 20992, 8, 2, 129, 435, 3281, 620, 29574, 37, 2, 1498, 5, 6533, 2, 24, 654, 17, 241, 887, 1965, 6, 3752, 65, 34, 21, 2308, 7452, 291, 8, 10, 495, 2221, 17, 58, 2, 7110, 3448, 5, 2666, 22219, 6, 13412, 21, 10541, 9245, 7, 3, 3753, 151, 4, 9, 2429, 323, 545, 48, 3, 1120, 11, 2666, 8704, 41, 2800, 10, 108, 5, 10542, 227, 27, 14, 631, 30, 5, 2, 78, 1039, 1036, 998, 755, 29, 11, 390, 35635, 76, 37, 2, 326, 5, 21, 1255, 8269, 7, 1079, 17, 3, 5199, 5, 6085, 1125, 21, 11352, 6, 10299, 4, 35638, 7, 100443, 23, 6, 710, 21, 316, 5, 479, 4, 411, 13812, 18, 227, 21, 69442, 20993, 599, 47, 6, 28, 2732, 2, 6821, 4, 4928, 8, 2, 513, 96, 36, 64, 25368, 18, 12, 114, 175, 37, 3, 393, 5, 32, 3894, 3517, 3388], 1)]\n",
      "[([868, 2313, 29392, 0, 442, 3, 76, 138, 20739, 5, 2, 502, 131, 1724, 68, 9, 9002, 48, 9, 2606, 636, 7, 11, 12, 14, 39, 38, 2338, 8, 158, 318, 70, 17859, 14000, 0, 70, 39, 0, 13, 558, 25, 37, 123, 11, 22, 400, 6, 166, 5447, 4, 81, 26, 869, 1976, 6, 63, 11, 1140, 7, 92312, 9, 98, 25, 74, 28, 1182, 6, 26, 119, 10, 3195, 4, 1865, 210, 5, 13350, 13, 2435, 3, 3873, 5, 815, 6, 3669, 428, 183, 12, 43, 32, 0, 2, 58, 307, 9, 4506, 285, 34, 1802, 7, 11, 12, 7, 3, 1519, 6128, 46, 25, 22, 8, 2, 1519, 16, 3, 61, 21792, 49, 714, 151, 96, 44, 3, 1317, 9, 355, 98, 44, 3, 39542, 18, 603, 5, 188, 64, 28, 8, 11, 1519, 35, 2, 390, 767, 1671, 0], 1), ([1292, 90413, 11644, 110, 10, 2004, 80, 151, 0, 29, 45, 1912, 44, 3, 138, 151, 18, 10, 30, 205, 97, 39, 12004, 13, 742, 1272, 7050, 249, 5, 797, 34, 32, 1382, 10638, 16256, 90851, 21795, 42797, 4, 2, 1128, 5, 8494, 454, 841, 91, 10, 32, 2224, 4, 26441, 4321, 24, 987, 2531, 35, 5, 2, 1116, 121, 504, 2, 68873, 5, 2, 1756, 1987, 17, 59, 5583, 49133, 30, 828, 7, 96790, 4, 7250, 3, 926, 5, 4004, 0, 31, 647, 35, 5, 21, 1448, 19126, 84, 4, 1387, 6, 21, 1768, 0, 0, 21, 378, 7, 3015, 4, 19969, 16, 14407, 4, 15421, 63, 53, 853, 554, 17, 42, 31505, 2741, 37003, 36, 239, 6, 422, 8, 59, 412, 17, 42, 15064, 5542, 10, 15064, 828, 7, 8971, 8, 120, 17, 21, 1687, 7032, 18, 14, 923, 72, 1756, 17, 3, 0, 6, 13450, 28289, 33, 58, 26, 454, 295, 50, 21, 6448, 2741, 378, 1900, 42, 251602, 13, 742, 7812, 517, 8, 2, 412, 4, 27, 853, 2594, 2, 1093, 11, 2, 103, 418, 22, 10682, 167, 2743, 122, 7, 2, 3937, 33049, 36, 7, 1965, 6, 1247, 40, 456, 16, 907, 610, 6, 3, 31531, 248, 53, 988, 3, 16162, 6675, 6, 6534, 42, 676, 15, 107, 15, 42, 14154, 17, 2, 256, 10056, 53, 1620, 115, 118, 140, 42, 6675, 40, 31, 17608, 42, 7054, 11, 53, 697, 0, 777, 162, 19, 8, 2, 412, 4, 79, 76, 53, 0, 13, 537, 2, 656, 2, 1205, 43, 1602, 239, 47, 4, 2, 103, 618, 2545, 172, 65, 578, 285, 1374, 712, 23, 179, 52, 7, 32, 869, 150, 187, 2, 0, 828, 4, 21, 12564, 812, 2959, 42, 202678, 4145, 19, 1340, 4, 29, 82, 69, 98, 53, 7, 162, 6, 5186, 248, 69, 67, 2, 103, 418, 2362, 8, 2, 49, 224, 150, 5, 2, 708, 13, 93, 3845, 5, 10, 24, 6216, 42, 55389, 25766, 18, 53, 287, 188, 2485, 396, 5133, 36, 22, 2768, 31, 59, 1701, 4, 24850, 69, 208, 6, 61, 456, 43, 2, 121, 4, 69, 86, 67, 115, 15, 10190, 233, 19, 3552, 188, 17, 59, 10636, 57263, 35711, 9493, 4, 8425], 1), ([10, 7, 57, 82, 216402, 0, 139, 9, 200, 2, 24, 19, 280, 8, 83, 6894, 324, 17, 83, 0, 122, 1394, 15, 0, 9, 81, 1271, 11, 12, 7, 20940, 36, 40892, 2, 18277, 4302, 31, 1340, 8, 2, 24, 4, 23, 0, 266, 48, 9, 403, 43, 2, 24, 14, 83, 1088, 31, 8022, 216403, 23, 42, 3123, 2, 646, 196, 4, 7665, 66, 20084, 4, 401, 155068, 854, 2, 0, 37, 2, 172, 5, 0, 11022, 33942, 61182, 33, 22, 2955, 418, 3483, 31, 3801, 3701, 4, 30, 3801, 160, 0, 175, 6, 208, 1989, 8, 2, 224, 339, 69, 67, 5, 0, 21, 58262, 542, 0, 27, 175, 6, 449, 180, 19, 3, 910, 5816, 675, 244102, 4, 0, 6, 8826, 109, 13, 80128, 0, 175, 6, 41315, 15, 3, 201, 31541, 1025, 6, 78, 1382, 2765, 627, 53, 54, 268, 6, 28, 150953, 15, 53, 486, 316, 42, 272, 6, 135, 675, 2, 73962, 10695, 256, 53, 7, 5535, 557, 5535, 3, 3067, 5, 2, 1687, 0, 8, 2, 994, 23108, 5, 11517, 53, 55, 46471, 2, 24, 6, 42, 542, 4, 778, 1472, 42, 0, 266, 42, 20940, 0, 5, 149611, 734, 17, 193, 42, 1018, 8, 3, 47624, 12488, 1633, 42, 6, 0, 84558, 0, 0, 0, 10000, 4, 53, 7, 43092, 31, 124, 0, 42, 3863, 2427, 411, 921, 4, 4209, 0, 191428, 24, 5, 103, 20338, 2391, 8, 32, 1382, 9789, 6237, 186, 28, 56515, 6, 7465, 18, 1400, 2200, 2, 0, 5, 9789, 9049, 11, 114, 23, 268, 6, 16896, 16, 143, 138, 68, 83, 5450, 5692, 8, 3, 1806, 994, 7079, 0, 0, 4, 0, 129, 26, 2957, 5328, 1521, 8, 4817, 18, 113, 23, 26, 2, 8244, 11, 3483, 10, 24, 4, 1934, 26, 23, 71, 119, 31, 3, 2205, 2614, 5, 17554, 0, 13, 80128, 97518, 8022, 0, 1722, 44063, 4, 1722, 125301, 22, 3917, 18, 23, 7588, 8022, 54565, 7, 3, 1039, 633, 36, 442, 1096, 423, 441, 56, 998, 0, 216054, 0, 241343, 0, 0, 0, 0, 3, 4850, 4134, 7109, 8, 10, 139, 8022, 26168, 0, 610, 6, 42, 387, 1558, 228, 68, 42, 132, 78795, 35, 8, 364, 2, 502, 2434, 1252, 8, 2, 3458, 5, 2, 1088, 63, 7, 801, 953, 8, 1403, 5, 1902, 1811, 9, 226, 274, 8022, 0, 64, 53472, 42, 553, 2447, 8, 42, 910, 59369], 1), ([10, 14, 3, 88, 24, 8, 158, 318, 5, 2, 6064, 12, 14409, 2, 961, 5, 0, 8, 3, 1333, 11, 7, 168, 44886, 5, 92, 46459, 34, 2, 29343, 12, 104671, 3, 88, 100, 1382, 14399, 11, 100, 153, 12790, 322, 130, 15, 2, 32900, 5, 16879, 31, 2751, 2, 4031, 5, 3564, 4, 3361, 1768, 60550, 59993, 2, 132989, 5, 418, 8, 1403, 5, 59, 0, 92282, 2, 37268, 8194, 5, 5707, 4, 38, 774, 29, 2, 657, 5, 2, 24, 7, 3, 1465, 120, 75, 11, 254, 675, 130, 8483, 5, 2, 1333, 63, 2, 103, 4213, 153, 2392, 2, 24, 7, 10064, 4, 19067, 302, 1157, 34, 505, 6, 656, 1030, 78, 3184, 7144, 2, 258, 8293, 5, 2, 24, 15, 9, 200, 12, 14, 1263, 11, 2, 103, 2545, 268, 1422, 6, 30, 167, 23, 2679, 31, 3, 1393, 15820, 16, 233, 80, 15, 76, 15, 2, 231, 11, 33, 22, 1499, 8, 26704, 16879, 17, 60, 2209, 4, 60, 80763, 10, 186, 304, 3, 182, 8, 2, 810, 8706, 5, 2, 646, 18, 78, 89, 1499, 8, 2, 163, 1173, 74, 23, 33671, 0, 12, 175, 827, 34, 2, 505, 5, 2, 24, 11, 2, 103, 121, 22, 168, 16049, 322, 50, 20940, 114, 42, 150, 29, 2, 172, 5, 2, 20, 17, 0, 53, 146, 12, 168, 827, 11, 41096, 1941, 53, 14, 30531, 4, 2, 13046, 14, 11, 46, 27, 62, 9486, 767, 27, 90, 26, 15563, 42, 168, 17836, 4, 81, 50, 0, 401, 49, 1084, 50, 42, 828, 401, 6, 23, 37, 717, 130, 1718, 85, 23, 453, 89, 72, 31404, 322, 33, 186, 2822, 5803, 8, 498, 8, 2, 163, 14511, 18, 0, 24, 12, 7, 23, 29, 35, 95, 827, 11, 33, 22, 17280, 34, 2, 505, 322, 168, 2, 8308, 45, 89, 22, 0, 12, 7, 2698, 18, 78, 2196, 6, 28, 452, 0, 40, 40327, 8, 2, 529, 5, 2, 2391, 8, 2, 151, 193, 62, 22137, 17802, 16, 7765, 421, 421, 421, 46, 10, 62, 23, 71, 2, 1707, 54, 33, 26, 3186, 236, 2, 311, 6, 2790, 59, 1941, 8, 3, 667, 11, 33, 186, 26, 1181, 23, 26, 0, 2, 24, 7891, 464, 2, 80, 2870, 7, 2, 13748, 5, 2, 121, 421, 421, 421, 2, 1652, 0, 4, 20940, 268, 3013, 4182, 6, 1787, 4, 15326, 31987, 3, 24, 8, 1416, 5236, 3, 1099, 667, 187, 3, 160, 699, 66871, 4, 167, 699, 0, 31, 695, 1652, 3409, 17, 898, 0, 68086, 2, 24, 7, 566, 221, 6, 814, 19, 83, 209, 4, 676, 60, 130, 7113, 8, 57, 3462, 29, 92, 8513, 2, 8293, 85, 23, 180, 76, 285, 34, 2, 818, 5, 2, 117, 12, 7, 1265, 3, 49, 1465, 4, 1057, 75, 322, 2, 1385, 4, 121, 74, 758, 17, 25, 3, 229, 84, 94, 25, 536, 2, 10270], 1), ([3, 6550, 7527, 151, 17, 4097, 590, 3932, 2488, 4, 60436, 13, 742, 45757, 554, 1116, 161, 23, 48, 53, 5868, 4, 458, 6, 1083, 42, 5707, 6, 907, 4472, 42, 5707, 6, 3814, 373, 17, 42, 1026, 0, 53, 146, 45, 7399, 88965, 34, 4507, 841, 4, 0], 1)]\n"
     ]
    }
   ],
   "source": [
    "# 把语料转换为id序列\n",
    "def convert_corpus_to_id(corpus, word2id_dict):\n",
    "    data_set = []\n",
    "    for sentence, sentence_label in corpus:\n",
    "        # 将句子中的词逐个替换成id，如果句子中的词不在词表内，则替换成oov\n",
    "        # 这里需要注意，一般来说我们可能需要查看一下test-set中，句子oov的比例，\n",
    "        # 如果存在过多oov的情况，那就说明我们的训练数据不足或者切分存在巨大偏差，需要调整\n",
    "        sentence = [word2id_dict[word] if word in word2id_dict \\\n",
    "                    else word2id_dict['[oov]'] for word in sentence]    \n",
    "        data_set.append((sentence, sentence_label))\n",
    "    return data_set\n",
    "\n",
    "train_corpus = convert_corpus_to_id(train_corpus, word2id_dict)\n",
    "test_corpus = convert_corpus_to_id(test_corpus, word2id_dict)\n",
    "print(\"%d tokens in the corpus\" % len(train_corpus))\n",
    "print(train_corpus[:5])\n",
    "print(test_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e78b3f9-70d5-48c5-9b5c-5ae6dcc4a10b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[[63923],\n",
      "        [44729],\n",
      "        [    2],\n",
      "        [ 1171],\n",
      "        [43012],\n",
      "        [    7],\n",
      "        [   30],\n",
      "        [    5],\n",
      "        [  136],\n",
      "        [  400],\n",
      "        [  170],\n",
      "        [  123],\n",
      "        [   43],\n",
      "        [  412],\n",
      "        [52426],\n",
      "        [ 1011],\n",
      "        [    3],\n",
      "        [  320],\n",
      "        [  707],\n",
      "        [    2],\n",
      "        [   88],\n",
      "        [  123],\n",
      "        [    5],\n",
      "        [   83],\n",
      "        [  840],\n",
      "        [    4],\n",
      "        [   55],\n",
      "        [    2],\n",
      "        [  627],\n",
      "        [  234]],\n",
      "\n",
      "       [[34913],\n",
      "        [26050],\n",
      "        [27037],\n",
      "        [ 6440],\n",
      "        [   57],\n",
      "        [ 1931],\n",
      "        [    3],\n",
      "        [   24],\n",
      "        [   11],\n",
      "        [26629],\n",
      "        [  800],\n",
      "        [   19],\n",
      "        [    2],\n",
      "        [19162],\n",
      "        [  667],\n",
      "        [  187],\n",
      "        [    2],\n",
      "        [15944],\n",
      "        [    4],\n",
      "        [   21],\n",
      "        [  629],\n",
      "        [11625],\n",
      "        [31786],\n",
      "        [   16],\n",
      "        [    3],\n",
      "        [   75],\n",
      "        [  189],\n",
      "        [   15],\n",
      "        [  946],\n",
      "        [   15]],\n",
      "\n",
      "       [[   15],\n",
      "        [  245],\n",
      "        [   15],\n",
      "        [    3],\n",
      "        [15920],\n",
      "        [    5],\n",
      "        [66723],\n",
      "        [   10],\n",
      "        [66171],\n",
      "        [ 1086],\n",
      "        [  407],\n",
      "        [   41],\n",
      "        [  105],\n",
      "        [  162],\n",
      "        [   16],\n",
      "        [  112],\n",
      "        [   44],\n",
      "        [ 2074],\n",
      "        [26272],\n",
      "        [    4],\n",
      "        [ 1003],\n",
      "        [11675],\n",
      "        [    6],\n",
      "        [ 8758],\n",
      "        [    2],\n",
      "        [ 1332],\n",
      "        [    4],\n",
      "        [ 1345],\n",
      "        [   11],\n",
      "        [    2]]], dtype=int64), array([[1],\n",
      "       [1],\n",
      "       [0]], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "# 编写一个迭代器，每次调用这个迭代器都会返回一个新的batch，用于训练或者预测\n",
    "def build_batch(word2id_dict, corpus, batch_size, epoch_num, max_seq_len, shuffle = True, drop_last = True):\n",
    "\n",
    "    # 模型将会接受的两个输入：\n",
    "    # 1. 一个形状为[batch_size, max_seq_len]的张量，sentence_batch，代表了一个mini-batch的句子。\n",
    "    # 2. 一个形状为[batch_size, 1]的张量，sentence_label_batch，每个元素都是非0即1，代表了每个句子的情感类别（正向或者负向）\n",
    "    sentence_batch = []\n",
    "    sentence_label_batch = []\n",
    "\n",
    "    for _ in range(epoch_num): \n",
    "\n",
    "        #每个epoch前都shuffle一下数据，有助于提高模型训练的效果\n",
    "        #但是对于预测任务，不要做数据shuffle\n",
    "        if shuffle:\n",
    "            random.shuffle(corpus)\n",
    "\n",
    "        for sentence, sentence_label in corpus:\n",
    "            sentence_sample = sentence[:min(max_seq_len, len(sentence))]\n",
    "            if len(sentence_sample) < max_seq_len:\n",
    "                for _ in range(max_seq_len - len(sentence_sample)):\n",
    "                    sentence_sample.append(word2id_dict['[pad]'])\n",
    "            \n",
    "            \n",
    "            sentence_sample = [[word_id] for word_id in sentence_sample]\n",
    "\n",
    "            sentence_batch.append(sentence_sample)\n",
    "            sentence_label_batch.append([sentence_label])\n",
    "\n",
    "            if len(sentence_batch) == batch_size:\n",
    "                yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "                sentence_batch = []\n",
    "                sentence_label_batch = []\n",
    "    if not drop_last and len(sentence_batch) > 0:\n",
    "        yield np.array(sentence_batch).astype(\"int64\"), np.array(sentence_label_batch).astype(\"int64\")\n",
    "\n",
    "for batch_id, batch in enumerate(build_batch(word2id_dict, train_corpus, batch_size=3, epoch_num=3, max_seq_len=30)):\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eda79bc2-0876-4d1f-bca5-d6609017cb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个用于情感分类的网络实例，SentimentClassifier\n",
    "class SentimentClassifier(paddle.nn.Layer):\n",
    "    \n",
    "    def __init__(self, hidden_size, vocab_size, embedding_size, class_num=2, num_steps=128, num_layers=1, init_scale=0.1, dropout_rate=None):\n",
    "        \n",
    "        # 参数含义如下：\n",
    "        # 1.hidden_size，表示embedding-size，hidden和cell向量的维度\n",
    "        # 2.vocab_size，模型可以考虑的词表大小\n",
    "        # 3.embedding_size，表示词向量的维度\n",
    "        # 4.class_num，情感类型个数，可以是2分类，也可以是多分类\n",
    "        # 5.num_steps，表示这个情感分析模型最大可以考虑的句子长度\n",
    "        # 6.num_layers，表示网络的层数\n",
    "        # 7.dropout_rate，表示使用dropout过程中失活的神经元比例\n",
    "        # 8.init_scale，表示网络内部的参数的初始化范围,长短时记忆网络内部用了很多Tanh，Sigmoid等激活函数，\\\n",
    "        # 这些函数对数值精度非常敏感，因此我们一般只使用比较小的初始化范围，以保证效果\n",
    "        super(SentimentClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.class_num = class_num\n",
    "        self.num_steps = num_steps\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.init_scale = init_scale\n",
    "       \n",
    "        # 声明一个LSTM模型，用来把每个句子抽象成向量\n",
    "        self.simple_lstm_rnn = paddle.nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers)\n",
    "\n",
    "        # 声明一个embedding层，用来把句子中的每个词转换为向量\n",
    "        self.embedding = paddle.nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_size, sparse=False, \n",
    "                                    weight_attr=paddle.ParamAttr(initializer=paddle.nn.initializer.Uniform(low=-init_scale, high=init_scale)))\n",
    "        \n",
    "        # 声明使用上述语义向量映射到具体情感类别时所需要使用的线性层\n",
    "        self.cls_fc = paddle.nn.Linear(in_features=self.hidden_size, out_features=self.class_num, \n",
    "                             weight_attr=None, bias_attr=None)\n",
    "        \n",
    "        # 一般在获取单词的embedding后，会使用dropout层，防止过拟合，提升模型泛化能力\n",
    "        self.dropout_layer = paddle.nn.Dropout(p=self.dropout_rate, mode='upscale_in_train')\n",
    "\n",
    "    # forwad函数即为模型前向计算的函数，它有两个输入，分别为：\n",
    "    # input为输入的训练文本，其shape为[batch_size, max_seq_len]\n",
    "    # label训练文本对应的情感标签，其shape维[batch_size, 1]\n",
    "    def forward(self, inputs):\n",
    "        # 获取输入数据的batch_size\n",
    "        batch_size = inputs.shape[0]\n",
    "\n",
    "        # 默认使用1层的LSTM，首先我们需要定义LSTM的初始hidden和cell，这里我们使用0来初始化这个序列的记忆\n",
    "        init_hidden_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\n",
    "        init_cell_data = np.zeros(\n",
    "            (self.num_layers, batch_size, self.hidden_size), dtype='float32')\n",
    "\n",
    "        # 将这些初始记忆转换为飞桨可计算的向量，并且设置stop_gradient=True，避免这些向量被更新，从而影响训练效果\n",
    "        init_hidden = paddle.to_tensor(init_hidden_data)\n",
    "        init_hidden.stop_gradient = True\n",
    "        init_cell = paddle.to_tensor(init_cell_data)\n",
    "        init_cell.stop_gradient = True\n",
    "\n",
    "        # 对应以上第2步，将输入的句子的mini-batch转换为词向量表示，转换后输入数据shape为[batch_size, max_seq_len, embedding_size]\n",
    "        x_emb = self.embedding(inputs)\n",
    "        x_emb = paddle.reshape(x_emb, shape=[-1, self.num_steps, self.embedding_size])\n",
    "        # 在获取的词向量后添加dropout层\n",
    "        if self.dropout_rate is not None and self.dropout_rate > 0.0:\n",
    "            x_emb = self.dropout_layer(x_emb)\n",
    "        \n",
    "        # 对应以上第3步，使用LSTM网络，把每个句子转换为语义向量\n",
    "        # 返回的last_hidden即为最后一个时间步的输出，其shape为[self.num_layers, batch_size, hidden_size]\n",
    "        rnn_out, (last_hidden, last_cell) = self.simple_lstm_rnn(x_emb, (init_hidden, init_cell))\n",
    "        # 提取最后一层隐状态作为文本的语义向量，其shape为[batch_size, hidden_size]\n",
    "        last_hidden = paddle.reshape(last_hidden[-1], shape=[-1, self.hidden_size])\n",
    "\n",
    "        # 对应以上第4步，将每个句子的向量表示映射到具体的情感类别上, logits的维度为[batch_size, 2]\n",
    "        logits = self.cls_fc(last_hidden)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d0b40e9-d3ed-440e-815f-f6b0a22bbcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss 0.687\n",
      "step 100, loss 0.687\n",
      "step 200, loss 0.691\n",
      "step 300, loss 0.698\n",
      "step 400, loss 0.662\n",
      "step 500, loss 0.481\n",
      "step 600, loss 0.261\n",
      "step 700, loss 0.271\n",
      "step 800, loss 0.167\n",
      "step 900, loss 0.093\n"
     ]
    }
   ],
   "source": [
    "paddle.seed(0)\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# 定义训练参数\n",
    "epoch_num = 5\n",
    "batch_size = 128\n",
    "\n",
    "learning_rate = 0.0001\n",
    "dropout_rate = 0.2\n",
    "num_layers = 1\n",
    "hidden_size = 256\n",
    "embedding_size = 256\n",
    "max_seq_len = 128\n",
    "vocab_size = len(word2id_freq)\n",
    "\n",
    "# 实例化模型\n",
    "sentiment_classifier = SentimentClassifier(hidden_size, vocab_size, embedding_size,  num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "\n",
    "# 指定优化策略，更新模型参数\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=learning_rate, beta1=0.9, beta2=0.999, parameters= sentiment_classifier.parameters()) \n",
    "\n",
    "# 定义训练函数\n",
    "# 记录训练过程中的损失变化情况，可用于后续画图查看训练情况\n",
    "losses = []\n",
    "steps = []\n",
    "\n",
    "def train(model):\n",
    "    # 开启模型训练模式\n",
    "    model.train()\n",
    "    \n",
    "    # 建立训练数据生成器，每次迭代生成一个batch，每个batch包含训练文本和文本对应的情感标签\n",
    "    train_loader = build_batch(word2id_dict, train_corpus, batch_size, epoch_num, max_seq_len)\n",
    "    \n",
    "    for step, (sentences, labels) in enumerate(train_loader):\n",
    "        # 获取数据，并将张量转换为Tensor类型\n",
    "        sentences = paddle.to_tensor(sentences)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        \n",
    "        # 前向计算，将数据feed进模型，并得到预测的标签和损失\n",
    "        logits = model(sentences)\n",
    "\n",
    "        # 计算损失\n",
    "        loss = F.cross_entropy(input=logits, label=labels, soft_label=False)\n",
    "        loss = paddle.mean(loss)\n",
    "\n",
    "        # 后向传播\n",
    "        loss.backward()\n",
    "        # 更新参数\n",
    "        optimizer.step()\n",
    "        # 清除梯度\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            # 记录当前步骤的loss变化情况\n",
    "            losses.append(loss.numpy())\n",
    "            steps.append(step)\n",
    "            # 打印当前loss数值\n",
    "            print(\"step %d, loss %.3f\" % (step, loss.numpy()))\n",
    "\n",
    "#训练模型\n",
    "train(sentiment_classifier)\n",
    "\n",
    "# 保存模型，包含两部分：模型参数和优化器参数\n",
    "model_name = \"sentiment_classifier\"\n",
    "# 保存训练好的模型参数\n",
    "paddle.save(sentiment_classifier.state_dict(), \"{}.pdparams\".format(model_name))\n",
    "# 保存优化器参数，方便后续模型继续训练\n",
    "paddle.save(optimizer.state_dict(), \"{}.pdopt\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4047ad76-b65e-4b25-a5c9-247ddf0fd5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 10337\n",
      "FP: 2343\n",
      "TN: 10137\n",
      "FN: 2143\n",
      "\n",
      "Accuracy: 0.8203\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model):\n",
    "    # 开启模型测试模式，在该模式下，网络不会进行梯度更新\n",
    "    model.eval()\n",
    "\n",
    "    # 定义以上几个统计指标\n",
    "    tp, tn, fp, fn = 0, 0, 0, 0\n",
    "\n",
    "    # 构造测试数据生成器\n",
    "    test_loader = build_batch(word2id_dict, test_corpus, batch_size, 1, max_seq_len)\n",
    "    \n",
    "    for sentences, labels in test_loader:\n",
    "        # 将张量转换为Tensor类型\n",
    "        sentences = paddle.to_tensor(sentences)\n",
    "        labels = paddle.to_tensor(labels)\n",
    "        \n",
    "        # 获取模型对当前batch的输出结果\n",
    "        logits = model(sentences)\n",
    "        \n",
    "        # 使用softmax进行归一化\n",
    "        probs = F.softmax(logits)\n",
    "\n",
    "        # 把输出结果转换为numpy array数组，比较预测结果和对应label之间的关系，并更新tp，tn，fp和fn\n",
    "        probs = probs.numpy()\n",
    "        for i in range(len(probs)):\n",
    "            # 当样本是的真实标签是正例\n",
    "            if labels[i][0] == 1:\n",
    "                # 模型预测是正例\n",
    "                if probs[i][1] > probs[i][0]:\n",
    "                    tp += 1\n",
    "                # 模型预测是负例\n",
    "                else:\n",
    "                    fn += 1\n",
    "            # 当样本的真实标签是负例\n",
    "            else:\n",
    "                # 模型预测是正例\n",
    "                if probs[i][1] > probs[i][0]:\n",
    "                    fp += 1\n",
    "                # 模型预测是负例\n",
    "                else:\n",
    "                    tn += 1\n",
    "\n",
    "    # 整体准确率\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    \n",
    "    # 输出最终评估的模型效果\n",
    "    print(\"TP: {}\\nFP: {}\\nTN: {}\\nFN: {}\\n\".format(tp, fp, tn, fn))\n",
    "    print(\"Accuracy: %.4f\" % accuracy)\n",
    "\n",
    "# 加载训练好的模型进行预测，重新实例化一个模型，然后将训练好的模型参数加载到新模型里面\n",
    "saved_state = paddle.load(\"./sentiment_classifier.pdparams\")\n",
    "sentiment_classifier = SentimentClassifier(hidden_size, vocab_size, embedding_size,  num_steps=max_seq_len, num_layers=num_layers, dropout_rate=dropout_rate)\n",
    "sentiment_classifier.load_dict(saved_state)\n",
    "\n",
    "# 评估模型\n",
    "evaluate(sentiment_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5cf38733-7c6a-4aa7-813b-f8391e7360a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    " \n",
    "data_base_path = r'aclImdb'\n",
    " \n",
    "train_batch_size = 64\n",
    "test_batch_size = 500\n",
    "max_len = 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    " \n",
    "# 分词的API\n",
    "def tokenize(text):\n",
    "    # fileters = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    fileters = ['!', '\"', '#', '$', '%', '&', '\\(', '\\)', '\\*', '\\+', ',', '-', '\\.', '/', ':', ';', '<', '=', '>',\n",
    "                '\\?', '@', '\\[', '\\\\', '\\]', '^', '_', '`', '\\{', '\\|', '\\}', '~', '\\t', '\\n', '\\x97', '\\x96', '”',\n",
    "                '“', ]\n",
    "    text = re.sub(\"<.*?>\", \" \", text, flags=re.S)\n",
    "    text = re.sub(\"|\".join(fileters), \" \", text, flags=re.S)\n",
    "    return [i.strip() for i in text.split()]\n",
    " \n",
    " \n",
    "# 自定义的数据集\n",
    "class ImdbDataset(Dataset):\n",
    "    def __init__(self, mode):\n",
    "        super(ImdbDataset, self).__init__()\n",
    "        if mode == \"train\":\n",
    "            text_path = [os.path.join(data_base_path, i) for i in [\"train/neg\", \"train/pos\"]]\n",
    "        else:\n",
    "            text_path = [os.path.join(data_base_path, i) for i in [\"test/neg\", \"test/pos\"]]\n",
    " \n",
    "        self.total_file_path_list = []\n",
    "        for i in text_path:\n",
    "            self.total_file_path_list.extend([os.path.join(i, j) for j in os.listdir(i)])\n",
    "        # print(self.total_file_path_list)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        cur_path = self.total_file_path_list[idx]\n",
    "        cur_filename = os.path.basename(cur_path)\n",
    "        label = int(cur_filename.split(\"_\")[-1].split(\".\")[0]) - 1\n",
    "        text = tokenize(open(cur_path, encoding=\"utf-8\").read().strip())\n",
    "        return label, text\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.total_file_path_list)\n",
    " \n",
    " \n",
    "# 自定义的collate_fn方法\n",
    "def collate_fn(batch):\n",
    "    batch = list(zip(*batch))\n",
    "    labels = torch.tensor(batch[0], dtype=torch.int32)\n",
    "    texts = batch[1]\n",
    "    texts = torch.tensor([ws.transform(i, max_len) for i in texts])\n",
    "    del batch\n",
    "    return labels.long(), texts.long()\n",
    " \n",
    " \n",
    "# 获取数据的方法\n",
    "def get_dataloader(train=True):\n",
    "    if train:\n",
    "        mode = 'train'\n",
    "    else:\n",
    "        mode = \"test\"\n",
    "    dataset = ImdbDataset(mode)\n",
    "    batch_size = train_batch_size if train else test_batch_size\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    " \n",
    " \n",
    "# Word2Sequence\n",
    "class Word2Sequence:\n",
    "    UNK_TAG = \"UNK\"\n",
    "    PAD_TAG = \"PAD\"\n",
    "    UNK = 0\n",
    "    PAD = 1\n",
    " \n",
    "    def __init__(self):\n",
    "        self.dict = {\n",
    "            self.UNK_TAG: self.UNK,\n",
    "            self.PAD_TAG: self.PAD\n",
    "        }\n",
    "        self.fited = False\n",
    "        self.count = {}\n",
    " \n",
    "    def to_index(self, word):\n",
    "        return self.dict.get(word, self.UNK)\n",
    " \n",
    "    def to_word(self, index):\n",
    "        if index in self.inversed_dict:\n",
    "            return self.inversed_dict[index]\n",
    "        return self.UNK_TAG\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.dict)\n",
    " \n",
    "    def fit(self, sentence):\n",
    "        for word in sentence:\n",
    "            self.count[word] = self.count.get(word, 0) + 1\n",
    " \n",
    "    def build_vocab(self, min_count=None, max_count=None, max_feature=None):\n",
    "        if min_count is not None:\n",
    "            self.count = {word: count for word, count in self.count.items() if count >= min_count}\n",
    " \n",
    "        if max_count is not None:\n",
    "            self.count = {word: count for word, count in self.count.items() if count <= max_count}\n",
    " \n",
    "        if max_feature is not None:\n",
    "            self.count = dict(sorted(self.count.items(), lambda x: x[-1], reverse=True)[:max_feature])\n",
    " \n",
    "        for word in self.count:\n",
    "            self.dict[word] = len(self.dict)\n",
    " \n",
    "        self.inversed_dict = dict(zip(self.dict.values(), self.dict.keys()))\n",
    " \n",
    "    def transform(self, sentence, max_len=None):\n",
    "        if max_len is not None:\n",
    "            r = [self.PAD] * max_len\n",
    "        else:\n",
    "            r = [self.PAD] * len(sentence)\n",
    "        if max_len is not None and len(sentence) > max_len:\n",
    "            sentence = sentence[:max_len]\n",
    "        for index, word in enumerate(sentence):\n",
    "            r[index] = self.to_index(word)\n",
    "        return np.array(r, dtype=np.int64)\n",
    " \n",
    "    def inverse_transform(self, indices):\n",
    "        sentence = []\n",
    "        for i in indices:\n",
    "            word = self.to_word(i)\n",
    "            sentence.append(word)\n",
    "        return sentence\n",
    " \n",
    " \n",
    "# 建立词表\n",
    "def fit_save_word_sequence():\n",
    "    word_to_sequence = Word2Sequence()\n",
    "    train_path = [os.path.join(data_base_path, i) for i in [\"train/neg\", \"train/pos\"]]\n",
    "    total_file_path_list = []\n",
    "    for i in train_path:\n",
    "        total_file_path_list.extend([os.path.join(i, j) for j in os.listdir(i)])\n",
    "    for cur_path in tqdm(total_file_path_list, ascii=True, desc=\"fitting\"):\n",
    "        word_to_sequence.fit(tokenize(open(cur_path, encoding=\"utf-8\").read().strip()))\n",
    "    word_to_sequence.build_vocab()\n",
    "    pickle.dump(word_to_sequence, open(\"model/ws.pkl\", \"wb\"))\n",
    " \n",
    " \n",
    "ws = pickle.load(open(\"./model/ws.pkl\", \"rb\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "431435e2-5c8c-4564-b8c4-9d0ff6c3823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fitting: 100%|#################################################################| 25000/25000 [00:11<00:00, 2157.61it/s]\n"
     ]
    }
   ],
   "source": [
    "fit_save_word_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b236855-ae32-4aaa-8755-bb5182ca4b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IMDBModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IMDBModel, self).__init__()\n",
    "        self.hidden_size = 64\n",
    "        self.embedding_dim = 200\n",
    "        self.num_layer = 2\n",
    "        self.bidirectional = True\n",
    "        self.bi_num = 2 if self.bidirectional else 1\n",
    "        self.dropout = 0.5\n",
    "        # 以上部分为超参数，可以自行修改\n",
    "        self.embedding = nn.Embedding(len(ws), self.embedding_dim, padding_idx=ws.PAD)\n",
    "        self.lstm = nn.LSTM(self.embedding_dim, self.hidden_size,\n",
    "                            self.num_layer, bidirectional=True, dropout=self.dropout)\n",
    "        self.fc = nn.Linear(self.hidden_size * self.bi_num, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # 进行轴交换\n",
    "        h_0, c_0 = self.init_hidden_state(x.size(1))\n",
    "        h_0 = h_0.to(x.device)  # 确保初始隐藏状态与输入数据在同一设备上\n",
    "        c_0 = c_0.to(x.device)  # 确保初始隐藏状态与输入数据在同一设备上\n",
    "        _, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        # 只要最后一个lstm单元处理的结果，取前向LSTM和后向LSTM的结果进行简单拼接\n",
    "        out = torch.cat([h_n[-2, :, :], h_n[-1, :, :]], dim=-1)\n",
    "        out = self.fc(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return F.log_softmax(out, dim=-1)\n",
    " \n",
    "    def init_hidden_state(self, batch_size):\n",
    "        h_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to(device)\n",
    "        c_0 = torch.rand(self.num_layer * self.bi_num, batch_size, self.hidden_size).to(device)\n",
    "        return h_0, c_0\n",
    "\n",
    " \n",
    " \n",
    "imdb_model = IMDBModel()\n",
    "optimizer = optim.Adam(imdb_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f494edc-7189-4190-84cf-3b526029f929",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    mode = True\n",
    "    train_dataloader = get_dataloader(mode)\n",
    "    for idx, (target, input) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = imdb_model(input)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, idx * len(input), len(train_dataloader.dataset),\n",
    "                       100. * idx / len(train_dataloader), loss.item()))\n",
    "            torch.save(imdb_model.state_dict(), \"model/mnist_net_lstm.pkl\")\n",
    "            torch.save(optimizer.state_dict(), 'model/mnist_optimizer_lstm.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec919623-59d7-445d-a5ff-bda42e7a1d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    mode = False\n",
    "    imdb_model.eval()\n",
    "    test_dataloader = get_dataloader(mode)\n",
    "    with torch.no_grad():\n",
    "        for target, input in test_dataloader:\n",
    "            output = imdb_model(input)\n",
    "            test_loss += F.nll_loss(output, target, reduction=\"sum\")\n",
    "            pred = torch.max(output, dim=-1, keepdim=False)[-1]\n",
    "            correct += pred.eq(target.data).sum()\n",
    "        test_loss = test_loss / len(test_dataloader.dataset)\n",
    "        print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_dataloader.dataset),\n",
    "            100. * correct / len(test_dataloader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81c3f406-96db-4115-88b9-e58f763cc9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.3230, Accuracy: 2363/25000 (9.45%)\n",
      "\n",
      "Train Epoch: 0 [0/25000 (0%)]\tLoss: 2.292537\n",
      "Train Epoch: 0 [640/25000 (3%)]\tLoss: 2.295325\n",
      "Train Epoch: 0 [1280/25000 (5%)]\tLoss: 2.125393\n",
      "Train Epoch: 0 [1920/25000 (8%)]\tLoss: 2.114346\n",
      "Train Epoch: 0 [2560/25000 (10%)]\tLoss: 2.066591\n",
      "Train Epoch: 0 [3200/25000 (13%)]\tLoss: 2.021529\n",
      "Train Epoch: 0 [3840/25000 (15%)]\tLoss: 2.025481\n",
      "Train Epoch: 0 [4480/25000 (18%)]\tLoss: 2.031007\n",
      "Train Epoch: 0 [5120/25000 (20%)]\tLoss: 2.088186\n",
      "Train Epoch: 0 [5760/25000 (23%)]\tLoss: 1.962016\n",
      "Train Epoch: 0 [6400/25000 (26%)]\tLoss: 2.051611\n",
      "Train Epoch: 0 [7040/25000 (28%)]\tLoss: 2.039484\n",
      "Train Epoch: 0 [7680/25000 (31%)]\tLoss: 2.030576\n",
      "Train Epoch: 0 [8320/25000 (33%)]\tLoss: 2.051902\n",
      "Train Epoch: 0 [8960/25000 (36%)]\tLoss: 2.061191\n",
      "Train Epoch: 0 [9600/25000 (38%)]\tLoss: 2.087222\n",
      "Train Epoch: 0 [10240/25000 (41%)]\tLoss: 1.946583\n",
      "Train Epoch: 0 [10880/25000 (43%)]\tLoss: 1.980494\n",
      "Train Epoch: 0 [11520/25000 (46%)]\tLoss: 1.904607\n",
      "Train Epoch: 0 [12160/25000 (49%)]\tLoss: 2.006650\n",
      "Train Epoch: 0 [12800/25000 (51%)]\tLoss: 2.009914\n",
      "Train Epoch: 0 [13440/25000 (54%)]\tLoss: 2.047030\n",
      "Train Epoch: 0 [14080/25000 (56%)]\tLoss: 1.940454\n",
      "Train Epoch: 0 [14720/25000 (59%)]\tLoss: 2.027821\n",
      "Train Epoch: 0 [15360/25000 (61%)]\tLoss: 2.038968\n",
      "Train Epoch: 0 [16000/25000 (64%)]\tLoss: 2.059745\n",
      "Train Epoch: 0 [16640/25000 (66%)]\tLoss: 1.995311\n",
      "Train Epoch: 0 [17280/25000 (69%)]\tLoss: 1.938234\n",
      "Train Epoch: 0 [17920/25000 (72%)]\tLoss: 2.014806\n",
      "Train Epoch: 0 [18560/25000 (74%)]\tLoss: 1.947943\n",
      "Train Epoch: 0 [19200/25000 (77%)]\tLoss: 1.870881\n",
      "Train Epoch: 0 [19840/25000 (79%)]\tLoss: 1.951592\n",
      "Train Epoch: 0 [20480/25000 (82%)]\tLoss: 1.935373\n",
      "Train Epoch: 0 [21120/25000 (84%)]\tLoss: 1.908937\n",
      "Train Epoch: 0 [21760/25000 (87%)]\tLoss: 1.968425\n",
      "Train Epoch: 0 [22400/25000 (90%)]\tLoss: 2.010285\n",
      "Train Epoch: 0 [23040/25000 (92%)]\tLoss: 2.040349\n",
      "Train Epoch: 0 [23680/25000 (95%)]\tLoss: 2.000190\n",
      "Train Epoch: 0 [24320/25000 (97%)]\tLoss: 1.988388\n",
      "Train Epoch: 0 [15600/25000 (100%)]\tLoss: 2.105933\n",
      "训练第1轮的测试结果-----------------------------------------------------------------------------------------\n",
      "\n",
      "Test set: Avg. loss: 1.9725, Accuracy: 5825/25000 (23.30%)\n",
      "\n",
      "Train Epoch: 1 [0/25000 (0%)]\tLoss: 1.972308\n",
      "Train Epoch: 1 [640/25000 (3%)]\tLoss: 1.885190\n",
      "Train Epoch: 1 [1280/25000 (5%)]\tLoss: 1.904619\n",
      "Train Epoch: 1 [1920/25000 (8%)]\tLoss: 1.891779\n",
      "Train Epoch: 1 [2560/25000 (10%)]\tLoss: 1.814046\n",
      "Train Epoch: 1 [3200/25000 (13%)]\tLoss: 1.824631\n",
      "Train Epoch: 1 [3840/25000 (15%)]\tLoss: 1.984325\n",
      "Train Epoch: 1 [4480/25000 (18%)]\tLoss: 1.836664\n",
      "Train Epoch: 1 [5120/25000 (20%)]\tLoss: 1.938866\n",
      "Train Epoch: 1 [5760/25000 (23%)]\tLoss: 1.984294\n",
      "Train Epoch: 1 [6400/25000 (26%)]\tLoss: 1.980181\n",
      "Train Epoch: 1 [7040/25000 (28%)]\tLoss: 1.966615\n",
      "Train Epoch: 1 [7680/25000 (31%)]\tLoss: 1.934172\n",
      "Train Epoch: 1 [8320/25000 (33%)]\tLoss: 1.902964\n",
      "Train Epoch: 1 [8960/25000 (36%)]\tLoss: 1.858279\n",
      "Train Epoch: 1 [9600/25000 (38%)]\tLoss: 1.988000\n",
      "Train Epoch: 1 [10240/25000 (41%)]\tLoss: 1.885110\n",
      "Train Epoch: 1 [10880/25000 (43%)]\tLoss: 1.937842\n",
      "Train Epoch: 1 [11520/25000 (46%)]\tLoss: 1.866675\n",
      "Train Epoch: 1 [12160/25000 (49%)]\tLoss: 1.879169\n",
      "Train Epoch: 1 [12800/25000 (51%)]\tLoss: 1.862837\n",
      "Train Epoch: 1 [13440/25000 (54%)]\tLoss: 1.893331\n",
      "Train Epoch: 1 [14080/25000 (56%)]\tLoss: 1.861038\n",
      "Train Epoch: 1 [14720/25000 (59%)]\tLoss: 1.796257\n",
      "Train Epoch: 1 [15360/25000 (61%)]\tLoss: 1.821893\n",
      "Train Epoch: 1 [16000/25000 (64%)]\tLoss: 1.960071\n",
      "Train Epoch: 1 [16640/25000 (66%)]\tLoss: 1.863954\n",
      "Train Epoch: 1 [17280/25000 (69%)]\tLoss: 1.851625\n",
      "Train Epoch: 1 [17920/25000 (72%)]\tLoss: 1.979245\n",
      "Train Epoch: 1 [18560/25000 (74%)]\tLoss: 1.884749\n",
      "Train Epoch: 1 [19200/25000 (77%)]\tLoss: 1.704994\n",
      "Train Epoch: 1 [19840/25000 (79%)]\tLoss: 1.884614\n",
      "Train Epoch: 1 [20480/25000 (82%)]\tLoss: 1.929013\n",
      "Train Epoch: 1 [21120/25000 (84%)]\tLoss: 1.822350\n",
      "Train Epoch: 1 [21760/25000 (87%)]\tLoss: 1.910469\n",
      "Train Epoch: 1 [22400/25000 (90%)]\tLoss: 1.906625\n",
      "Train Epoch: 1 [23040/25000 (92%)]\tLoss: 1.696872\n",
      "Train Epoch: 1 [23680/25000 (95%)]\tLoss: 1.801683\n",
      "Train Epoch: 1 [24320/25000 (97%)]\tLoss: 1.863474\n",
      "Train Epoch: 1 [15600/25000 (100%)]\tLoss: 1.809910\n",
      "训练第2轮的测试结果-----------------------------------------------------------------------------------------\n",
      "\n",
      "Test set: Avg. loss: 1.8695, Accuracy: 7634/25000 (30.54%)\n",
      "\n",
      "Train Epoch: 2 [0/25000 (0%)]\tLoss: 1.814650\n",
      "Train Epoch: 2 [640/25000 (3%)]\tLoss: 1.545296\n",
      "Train Epoch: 2 [1280/25000 (5%)]\tLoss: 1.560049\n",
      "Train Epoch: 2 [1920/25000 (8%)]\tLoss: 1.792138\n",
      "Train Epoch: 2 [2560/25000 (10%)]\tLoss: 1.815760\n",
      "Train Epoch: 2 [3200/25000 (13%)]\tLoss: 1.824004\n",
      "Train Epoch: 2 [3840/25000 (15%)]\tLoss: 1.802829\n",
      "Train Epoch: 2 [4480/25000 (18%)]\tLoss: 1.554429\n",
      "Train Epoch: 2 [5120/25000 (20%)]\tLoss: 1.717432\n",
      "Train Epoch: 2 [5760/25000 (23%)]\tLoss: 1.752027\n",
      "Train Epoch: 2 [6400/25000 (26%)]\tLoss: 1.680068\n",
      "Train Epoch: 2 [7040/25000 (28%)]\tLoss: 1.752026\n",
      "Train Epoch: 2 [7680/25000 (31%)]\tLoss: 1.760242\n",
      "Train Epoch: 2 [8320/25000 (33%)]\tLoss: 1.644746\n",
      "Train Epoch: 2 [8960/25000 (36%)]\tLoss: 1.690303\n",
      "Train Epoch: 2 [9600/25000 (38%)]\tLoss: 1.821452\n",
      "Train Epoch: 2 [10240/25000 (41%)]\tLoss: 1.677135\n",
      "Train Epoch: 2 [10880/25000 (43%)]\tLoss: 1.585667\n",
      "Train Epoch: 2 [11520/25000 (46%)]\tLoss: 1.647643\n",
      "Train Epoch: 2 [12160/25000 (49%)]\tLoss: 1.647313\n",
      "Train Epoch: 2 [12800/25000 (51%)]\tLoss: 1.692665\n",
      "Train Epoch: 2 [13440/25000 (54%)]\tLoss: 1.435495\n",
      "Train Epoch: 2 [14080/25000 (56%)]\tLoss: 1.783298\n",
      "Train Epoch: 2 [14720/25000 (59%)]\tLoss: 1.497872\n",
      "Train Epoch: 2 [15360/25000 (61%)]\tLoss: 1.664410\n",
      "Train Epoch: 2 [16000/25000 (64%)]\tLoss: 1.837672\n",
      "Train Epoch: 2 [16640/25000 (66%)]\tLoss: 1.560879\n",
      "Train Epoch: 2 [17280/25000 (69%)]\tLoss: 1.680584\n",
      "Train Epoch: 2 [17920/25000 (72%)]\tLoss: 1.584353\n",
      "Train Epoch: 2 [18560/25000 (74%)]\tLoss: 1.579208\n",
      "Train Epoch: 2 [19200/25000 (77%)]\tLoss: 1.555805\n",
      "Train Epoch: 2 [19840/25000 (79%)]\tLoss: 1.714331\n",
      "Train Epoch: 2 [20480/25000 (82%)]\tLoss: 1.635765\n",
      "Train Epoch: 2 [21120/25000 (84%)]\tLoss: 1.756956\n",
      "Train Epoch: 2 [21760/25000 (87%)]\tLoss: 1.654457\n",
      "Train Epoch: 2 [22400/25000 (90%)]\tLoss: 1.532077\n",
      "Train Epoch: 2 [23040/25000 (92%)]\tLoss: 1.628475\n",
      "Train Epoch: 2 [23680/25000 (95%)]\tLoss: 1.627677\n",
      "Train Epoch: 2 [24320/25000 (97%)]\tLoss: 1.677120\n",
      "Train Epoch: 2 [15600/25000 (100%)]\tLoss: 1.436462\n",
      "训练第3轮的测试结果-----------------------------------------------------------------------------------------\n",
      "\n",
      "Test set: Avg. loss: 1.8339, Accuracy: 7846/25000 (31.38%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    # # 测试数据集的功能\n",
    "\n",
    " \n",
    "    # 训练和测试\n",
    "    test()\n",
    "    for i in range(3):\n",
    "        train(i)\n",
    "        print(\n",
    "            \"训练第{}轮的测试结果-----------------------------------------------------------------------------------------\".format(\n",
    "                i + 1))\n",
    "        test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d067b53-4162-42c6-be61-f082216c3c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b585c28-9408-4526-8c48-6541d23b32cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\78658\\.conda\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import senteval\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from six.moves import cPickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a327f354-f68b-4f1d-8cc8-48e7fa75a61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型加载成功！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 检查模型是否能够成功加载\n",
    "try:\n",
    "    model = torch.load('model/mnist_net_lstm.pkl')\n",
    "    print(\"模型加载成功！\")\n",
    "except Exception as e:\n",
    "    print(\"模型加载失败:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07b5d0a-7ff2-4cf5-a82e-9558a6e40190",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e37e25ba-818f-4a8f-9bca-8f3635614d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb49aa4c-0ba6-4ab3-9210-a79308416d82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 3 fields in line 1235, saw 4\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 加载STSB数据集\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/78658/SentEval-main/data/downstream/STS/STSBenchmark/sts-train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcolumn2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 替换'column1', 'column2'为实际的列名\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m dev_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-dev.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 3 fields in line 1235, saw 4\n"
     ]
    }
   ],
   "source": [
    "# 加载STSB数据集\n",
    "train_df = pd.read_csv('C:/Users/78658/SentEval-main/data/downstream/STS/STSBenchmark/sts-train.csv', \n",
    "                       delimiter=',', names=['column1', 'column2', ...])  # 替换'column1', 'column2'为实际的列名\n",
    "test_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-test.csv')\n",
    "dev_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-dev.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d02085f7-601a-48dc-8463-03b1d4288bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 104, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorDataset, DataLoader\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# 加载dev、test和train数据集\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m dev_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m78658\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSentEval-main\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdownstream\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSTS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSTSBenchmark\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msts-dev.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 104, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# 加载dev、test和train数据集\n",
    "dev_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-dev.csv')\n",
    "test_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-test.csv')\n",
    "train_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-train.csv')\n",
    "\n",
    "# 使用BertTokenizer对文本进行编码\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_text(df, tokenizer):\n",
    "    encodings = tokenizer(df['sentence1'].tolist(), df['sentence2'].tolist(), padding=True, truncation=True, return_tensors='pt')\n",
    "    labels = torch.tensor(df['similarity_score'].tolist())\n",
    "    dataset = TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# 对dev、test和train数据集进行编码\n",
    "dev_dataset = encode_text(dev_df, tokenizer)\n",
    "test_dataset = encode_text(test_df, tokenizer)\n",
    "train_dataset = encode_text(train_df, tokenizer)\n",
    "\n",
    "# 定义批处理大小并创建数据加载器\n",
    "batch_size = 32\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 现在可以使用dev_loader、test_loader和train_loader对模型进行训练和评估\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e58ac799-ede5-4227-a467-265935bb00c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 104, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 加载dev、test和train数据集\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m dev_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m78658\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSentEval-main\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mdownstream\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSTS\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mSTSBenchmark\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msts-dev.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      7\u001b[0m train_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m78658\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSentEval-main\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdownstream\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTS\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSTSBenchmark\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124msts-train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 104, saw 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "# 加载dev、test和train数据集\n",
    "dev_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-dev.csv', delimiter=',')\n",
    "test_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-test.csv', delimiter=',')\n",
    "train_df = pd.read_csv(r'C:\\Users\\78658\\SentEval-main\\data\\downstream\\STS\\STSBenchmark\\sts-train.csv', delimiter=',')\n",
    "\n",
    "# 示例：使用正则表达式删除换行符\n",
    "# 修正为对加载的具体数据集进行操作\n",
    "dev_df['column_name'] = dev_df['column_name'].apply(lambda x: re.sub(r'\\n', '', str(x)))\n",
    "\n",
    "# 数据清理的示例步骤\n",
    "def tokenize(text):\n",
    "    # 定义需要过滤的字符列表\n",
    "    filters = ['!', '\"', '#', '$', '%', '&', '\\(', '\\)', '\\*', '\\+', ',', '-', '\\.', '/', ':', ';', '<', '=', '>',\n",
    "                '\\?', '@', '\\[', '\\\\', '\\]', '^', '_', '`', '\\{', '\\|', '\\}', '~', '\\t', '\\n', '\\x97', '\\x96', '”',\n",
    "                '“']\n",
    "\n",
    "    # 使用正则表达式进行文本预处理\n",
    "    text = re.sub(\"<.*?>\", \" \", str(text), flags=re.S)\n",
    "    text = re.sub(\"|\".join(filters), \" \", str(text), flags=re.S)\n",
    "    return [i.strip() for i in str(text).split()]\n",
    "\n",
    "# 数据清理的示例步骤\n",
    "def clean_dataset(df):\n",
    "    # 处理缺失值\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 处理重复值\n",
    "    df.drop_duplicates(inplace=True)\n",
    "\n",
    "    # 示例: 对特定列进行文本预处理\n",
    "    df['text_column'] = df['text_column'].apply(tokenize)\n",
    "  \n",
    "    # 进行其他清理操作\n",
    "\n",
    "    return df\n",
    "\n",
    "# 对每个数据集文件进行数据清理\n",
    "cleaned_dev_df = clean_dataset(dev_df)\n",
    "cleaned_test_df = clean_dataset(test_df)\n",
    "cleaned_train_df = clean_dataset(train_df)\n",
    "\n",
    "# 保存处理后的数据集到新文件\n",
    "cleaned_dev_df.to_csv('cleaned_dev_dataset.csv', index=False)\n",
    "cleaned_test_df.to_csv('cleaned_test_dataset.csv', index=False)\n",
    "cleaned_train_df.to_csv('cleaned_train_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21763aff-b184-4367-babc-f5317584f91b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
